<story-context id="bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>1</epicId>
    <storyId>2</storyId>
    <title>PostgreSQL + pgvector Setup</title>
    <status>drafted</status>
    <generatedAt>2025-11-11</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>bmad-docs/stories/1-2-postgresql-pgvector-setup.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>Entwickler</asA>
    <iWant>PostgreSQL mit pgvector-Extension lokal aufsetzen</iWant>
    <soThat>ich Embeddings (1536-dimensional) effizient speichern und durchsuchen kann</soThat>
    <tasks>
- PostgreSQL Installation prüfen/durchführen (AC: 1)
  - Arch Linux: sudo pacman -S postgresql (falls nicht installiert)
  - PostgreSQL Version verifizieren: psql --version (muss 15+ sein)
  - PostgreSQL initialisieren: sudo -u postgres initdb -D /var/lib/postgres/data
  - PostgreSQL Service starten: sudo systemctl start postgresql
  - PostgreSQL Service enablen: sudo systemctl enable postgresql
  - Service Status prüfen: systemctl status postgresql → "active (running)"

- pgvector Extension installieren (AC: 1)
  - Option A (empfohlen): AUR Package nutzen: yay -S pgvector
  - Option B: From Source (git clone + make + sudo make install)
  - Verifizieren: Extension-Dateien in /usr/lib/postgresql/ vorhanden

- Datenbank und User erstellen (AC: 1)
  - PostgreSQL Shell öffnen: sudo -u postgres psql
  - Datenbank erstellen: CREATE DATABASE cognitive_memory;
  - User erstellen: CREATE USER mcp_user WITH PASSWORD 'secure_password';
  - Rechte vergeben: GRANT ALL PRIVILEGES ON DATABASE cognitive_memory TO mcp_user;
  - pgvector Extension aktivieren: \c cognitive_memory dann CREATE EXTENSION vector;
  - Extension-Status prüfen: SELECT * FROM pg_extension WHERE extname='vector';

- Migration-Script erstellen und ausführen (AC: 2, 3)
  - Migration-File erstellen: mcp_server/db/migrations/001_initial_schema.sql
  - Alle 6 Tabellen-Definitionen einfügen
  - Alle Indizes definieren (IVFFlat als COMMENT, GIN/Session/LRU sofort bauen)
  - SQL-Syntax validieren
  - Migration ausführen: psql -U mcp_user -d cognitive_memory -f ...
  - Schema-Validierung: \dt → 6 Tabellen sichtbar
  - Index-Validierung: \di → 3 Indizes vorhanden

- Python Connection-Test (AC: 4)
  - Test-Script erstellen: tests/test_database.py
  - .env.development aktualisieren: POSTGRES_* Variablen
  - Connection-Test mit psycopg2.connect()
  - Test-Query: SELECT 1
  - pgvector Extension prüfen
  - WRITE-Test: INSERT + DELETE in l0_raw
  - Vector-Operation Test: Cosine Similarity Query in l2_insights

- Dokumentation aktualisieren (AC: 1, 2, 3, 4)
  - README.md: PostgreSQL Setup-Anleitung
  - pgvector Installation (AUR + Source)
  - Migration-Prozess
  - IVFFlat Index Build-Strategie
  - Troubleshooting: PostgreSQL-Fehler
  - Environment Variables dokumentieren
</tasks>
  </story>

  <acceptanceCriteria>
**Given** eine lokale Entwicklungsumgebung (Story 1.1 abgeschlossen)
**When** ich PostgreSQL + pgvector installiere und konfiguriere
**Then** ist folgendes Setup vorhanden:

1. **PostgreSQL Installation und Konfiguration**
   - PostgreSQL 15+ läuft lokal (Port 5432)
   - Service status: systemctl status postgresql zeigt "active (running)"
   - pgvector Extension ist installiert und aktiviert
   - Datenbank cognitive_memory existiert
   - User mcp_user mit Passwort existiert und hat entsprechende Rechte

2. **Datenbank-Schema vollständig**
   - l0_raw Tabelle (id, session_id, timestamp, speaker, content, metadata)
   - l2_insights Tabelle (id, content, embedding vector(1536), created_at, source_ids, metadata)
   - working_memory Tabelle (id, content, importance, last_accessed, created_at)
   - episode_memory Tabelle (id, query, reward, reflection, created_at, embedding vector(1536))
   - stale_memory Tabelle (id, original_content, archived_at, importance, reason)
   - ground_truth Tabelle (id, query, expected_docs, judge1_score, judge2_score, judge1_model, judge2_model, kappa, created_at)

3. **Indizes korrekt vorbereitet**
   - IVFFlat-Index SQL definiert in Migration (für l2_insights.embedding und episode_memory.embedding)
   - ⚠️ WICHTIG: IVFFlat-Indizes werden NICHT sofort gebaut (pgvector benötigt ≥100 Vektoren für Training)
   - Index-Build erfolgt später in Story 1.5 nach ersten Daten-Inserts
   - Full-Text Search Index (GIN) für l2_insights.content erstellt
   - Session-Index für l0_raw (session_id, timestamp) erstellt
   - LRU-Index für working_memory (last_accessed) erstellt

4. **Python-Connection funktioniert**
   - psycopg2-Connection erfolgreich (psycopg2.connect())
   - Test-Query erfolgreich (SELECT 1)
   - pgvector Extension verfügbar (SELECT * FROM pg_extension WHERE extname='vector')
   - WRITE-Test erfolgreich (INSERT INTO l0_raw, dann DELETE)
   - Vector-Operation funktioniert (INSERT vector(1536) in l2_insights, dann Cosine Similarity Query)
</acceptanceCriteria>

  <artifacts>
    <docs>
      <doc>
        <path>bmad-docs/architecture.md</path>
        <title>System Architecture</title>
        <section>Database Schema (lines 206-330)</section>
        <snippet>Vollständige SQL-Definitionen für alle 6 Tabellen (l0_raw, l2_insights, working_memory, episode_memory, stale_memory, ground_truth) mit Datentypen, Indizes und Constraints. Enthält IVFFlat Index Konfiguration (lists=100) und GIN Full-Text Search Index.</snippet>
      </doc>
      <doc>
        <path>bmad-docs/architecture.md</path>
        <title>System Architecture</title>
        <section>Tech Stack (line 27)</section>
        <snippet>PostgreSQL 15+ mit pgvector Extension für native Vektor-Suche. IVFFlat Index optimiert für &lt;100k Vektoren mit lists=100 Parameter. Production-Ready Setup.</snippet>
      </doc>
      <doc>
        <path>bmad-docs/architecture.md</path>
        <title>System Architecture</title>
        <section>ADR-003: PostgreSQL + pgvector (lines 788-804)</section>
        <snippet>Architectural Decision: PostgreSQL + pgvector statt spezialisierte Vektor-DB. Rationale: Production-Ready Reliability, Single Database (keine separate Vektor-DB), Native SQL für Hybrid Search.</snippet>
      </doc>
      <doc>
        <path>bmad-docs/architecture.md</path>
        <title>System Architecture</title>
        <section>ADR-004: IVFFlat Index (lines 806-811)</section>
        <snippet>IVFFlat Index (lists=100) statt HNSW für pgvector. Balance zwischen Speed und Accuracy für &lt;100k Vektoren. Query-Time &lt;100ms für Top-K Retrieval (p95).</snippet>
      </doc>
      <doc>
        <path>bmad-docs/architecture.md</path>
        <title>System Architecture</title>
        <section>Installation Commands (lines 686-698)</section>
        <snippet>Arch Linux Setup: sudo pacman -S postgresql, pgvector from source (git clone + make + sudo make install), Database Initialization mit initdb.</snippet>
      </doc>
      <doc>
        <path>bmad-docs/tech-spec-epic-1.md</path>
        <title>Technical Specification - Epic 1</title>
        <section>AC-1.2: PostgreSQL Setup (lines 662-671)</section>
        <snippet>Acceptance Criteria: PostgreSQL 15+ läuft lokal, pgvector Extension installiert, cognitive_memory Datenbank und mcp_user existieren, alle 6 Tabellen erstellt, IVFFlat und GIN Indizes vorhanden, Python Connection erfolgreich.</snippet>
      </doc>
      <doc>
        <path>bmad-docs/PRD.md</path>
        <title>Product Requirements Document</title>
        <section>Technical Architecture - Database (lines 377-379)</section>
        <snippet>PostgreSQL + pgvector mit 6 Tabellen (l0_raw, l2_insights, working_memory, episode_memory, stale_memory, ground_truth). Version 3.1 fügt judge1_model und judge2_model Spalten zur ground_truth Tabelle hinzu.</snippet>
      </doc>
      <doc>
        <path>bmad-docs/epics.md</path>
        <title>Epic Breakdown</title>
        <section>Story 1.2 Definition (lines 88-122)</section>
        <snippet>User Story: Als Entwickler möchte ich PostgreSQL mit pgvector lokal aufsetzen für effiziente 1536-dimensionale Embedding-Speicherung. Prerequisites: Story 1.1. Technical Notes: IVFFlat Index mit lists=100, Migration-Scripts in /migrations/.</snippet>
      </doc>
    </docs>
    <code>
      <artifact>
        <path>mcp_server/__init__.py</path>
        <kind>package</kind>
        <symbol>mcp_server</symbol>
        <lines>1</lines>
        <reason>Haupt-Package für MCP Server, aktuell nur __init__.py vorhanden (Story 1.1). Weitere Module werden in Story 1.3 hinzugefügt.</reason>
      </artifact>
      <artifact>
        <path>mcp_server/db/__init__.py</path>
        <kind>package</kind>
        <symbol>mcp_server.db</symbol>
        <lines>1</lines>
        <reason>Database-Package, aktuell leer. Story 1.2 erstellt hier migrations/ Ordner. connection.py wird NICHT in Story 1.2 erstellt (erst Story 1.3).</reason>
      </artifact>
      <artifact>
        <path>config/config.yaml</path>
        <kind>config</kind>
        <symbol>database.development</symbol>
        <lines>56-66</lines>
        <reason>Development Database Config mit localhost, Port 5432, cognitive_memory DB, mcp_user. Password kommt aus .env.development.</reason>
      </artifact>
      <artifact>
        <path>.env.development</path>
        <kind>config</kind>
        <symbol>POSTGRES_*</symbol>
        <lines>24-32</lines>
        <reason>PostgreSQL Credentials Template (POSTGRES_HOST, PORT, DB, USER, PASSWORD). WICHTIG: File ist im PROJECT ROOT, nicht in config/. chmod 600 bereits gesetzt (Story 1.1).</reason>
      </artifact>
      <artifact>
        <path>pyproject.toml</path>
        <kind>config</kind>
        <symbol>tool.poetry.dependencies</symbol>
        <lines>9-19</lines>
        <reason>Dependencies: psycopg2-binary (^2.9.0) und pgvector (^0.2.0) bereits definiert (Story 1.1). python-dotenv für .env Loading.</reason>
      </artifact>
      <artifact>
        <path>tests/__init__.py</path>
        <kind>test</kind>
        <symbol>tests</symbol>
        <lines>1</lines>
        <reason>Test-Package bereit für test_database.py (Story 1.2 erstellt Connection-Tests hier).</reason>
      </artifact>
    </code>
    <dependencies>
      <python>
        <package name="psycopg2-binary" version="^2.9.0" ecosystem="PyPI">PostgreSQL adapter für Python, binary distribution (keine Compilation erforderlich)</package>
        <package name="pgvector" version="^0.2.0" ecosystem="PyPI">Python Client für pgvector Extension, vector operations</package>
        <package name="python-dotenv" version="^1.0.0" ecosystem="PyPI">.env File Loading für Environment Variables</package>
        <package name="numpy" version="^1.24.0" ecosystem="PyPI">Numerische Operations für Vektor-Handling</package>
      </python>
      <system>
        <package name="postgresql" version="15+" ecosystem="Arch Linux pacman">PostgreSQL Datenbank Server (postgresql-contrib inkludiert in Arch)</package>
        <package name="pgvector" version="latest" ecosystem="AUR oder Source">pgvector Extension für PostgreSQL (Option A: yay -S pgvector, Option B: git clone + make install)</package>
      </system>
    </dependencies>
  </artifacts>

  <constraints>
    <constraint>UUID-Generierung erfolgt CLIENT-SIDE (Python uuid.uuid4()) statt Server-side DEFAULT. Rationale: MCP Tools generieren UUIDs vor INSERT für bessere Kontrolle, kein DB-Roundtrip erforderlich. Migration aktiviert uuid-ossp Extension für optionale Server-side Generation.</constraint>
    <constraint>IVFFlat-Indizes können NICHT sofort gebaut werden - pgvector benötigt mindestens 100 Vektoren für Index-Training. Lösung: Index-Definition als COMMENTED SQL in Migration-Script. Index-Build erfolgt später in Story 1.5 nach ersten Daten-Inserts.</constraint>
    <constraint>connection.py Modul ist OUT OF SCOPE für Story 1.2. Connection Pool wird in Story 1.3 (MCP Server Grundstruktur) erstellt. Story 1.2 nutzt direkte psycopg2.connect() Calls in tests/ für DB Setup Validation.</constraint>
    <constraint>.env Files (.env.development, .env.production) befinden sich im PROJECT ROOT, nicht in config/. python-dotenv lädt mit load_dotenv('.env.development') aus ROOT. File Permissions: chmod 600 (nur Owner readable).</constraint>
    <constraint>Type Hints sind ERFORDERLICH für alle Python-Funktionen (mypy strict mode aktiviert in pyproject.toml). disallow_untyped_defs=true, disallow_incomplete_defs=true.</constraint>
    <constraint>Code Style: Black (line-length=88) + Ruff (pycodestyle, pyflakes, isort, flake8-bugbear). Pre-commit hooks validieren automatisch (Story 1.1 Setup).</constraint>
    <constraint>SQL-Syntax MUSS validiert werden vor Migration-Ausführung (Learning aus Story 1.1: config.yaml hatte YAML-Syntax-Fehler). Manuelle Review + optional psql dry-run auf Test-DB.</constraint>
    <constraint>PostgreSQL 15+ ist MINIMUM-Version (für pgvector Compatibility und IVFFlat Index Support). Version verifizieren mit: psql --version.</constraint>
  </constraints>

  <interfaces>
    <interface>
      <name>l0_raw Table</name>
      <kind>PostgreSQL Table</kind>
      <signature>CREATE TABLE l0_raw (id SERIAL PRIMARY KEY, session_id UUID NOT NULL, timestamp TIMESTAMPTZ DEFAULT NOW(), speaker VARCHAR(50) NOT NULL, content TEXT NOT NULL, metadata JSONB); CREATE INDEX idx_l0_session ON l0_raw(session_id, timestamp);</signature>
      <path>bmad-docs/architecture.md:206-217</path>
    </interface>
    <interface>
      <name>l2_insights Table</name>
      <kind>PostgreSQL Table</kind>
      <signature>CREATE TABLE l2_insights (id SERIAL PRIMARY KEY, content TEXT NOT NULL, embedding vector(1536) NOT NULL, created_at TIMESTAMPTZ DEFAULT NOW(), source_ids INTEGER[] NOT NULL, metadata JSONB); CREATE INDEX idx_l2_fts ON l2_insights USING gin(to_tsvector('english', content)); -- IVFFlat Index COMMENTED (needs training data)</signature>
      <path>bmad-docs/architecture.md:219-231</path>
    </interface>
    <interface>
      <name>working_memory Table</name>
      <kind>PostgreSQL Table</kind>
      <signature>CREATE TABLE working_memory (id SERIAL PRIMARY KEY, content TEXT NOT NULL, importance FLOAT DEFAULT 0.5, last_accessed TIMESTAMPTZ DEFAULT NOW(), created_at TIMESTAMPTZ DEFAULT NOW()); CREATE INDEX idx_wm_lru ON working_memory(last_accessed ASC);</signature>
      <path>bmad-docs/architecture.md:233-243</path>
    </interface>
    <interface>
      <name>episode_memory Table</name>
      <kind>PostgreSQL Table</kind>
      <signature>CREATE TABLE episode_memory (id SERIAL PRIMARY KEY, query TEXT NOT NULL, reward FLOAT NOT NULL, reflection TEXT NOT NULL, created_at TIMESTAMPTZ DEFAULT NOW(), embedding vector(1536) NOT NULL); -- IVFFlat Index COMMENTED (needs training data)</signature>
      <path>bmad-docs/architecture.md:245-256</path>
    </interface>
    <interface>
      <name>stale_memory Table</name>
      <kind>PostgreSQL Table</kind>
      <signature>CREATE TABLE stale_memory (id SERIAL PRIMARY KEY, original_content TEXT NOT NULL, archived_at TIMESTAMPTZ DEFAULT NOW(), importance FLOAT NOT NULL, reason VARCHAR(100) NOT NULL);</signature>
      <path>bmad-docs/architecture.md:258-267</path>
    </interface>
    <interface>
      <name>ground_truth Table</name>
      <kind>PostgreSQL Table</kind>
      <signature>CREATE TABLE ground_truth (id SERIAL PRIMARY KEY, query TEXT NOT NULL, expected_docs INTEGER[] NOT NULL, judge1_score FLOAT, judge2_score FLOAT, judge1_model VARCHAR(100), judge2_model VARCHAR(100), kappa FLOAT, created_at TIMESTAMPTZ DEFAULT NOW());</signature>
      <path>bmad-docs/architecture.md:269-282</path>
    </interface>
    <interface>
      <name>psycopg2 Connection</name>
      <kind>Python Function</kind>
      <signature>psycopg2.connect(host=os.getenv('POSTGRES_HOST'), port=os.getenv('POSTGRES_PORT'), database=os.getenv('POSTGRES_DB'), user=os.getenv('POSTGRES_USER'), password=os.getenv('POSTGRES_PASSWORD'))</signature>
      <path>bmad-docs/stories/1-2-postgresql-pgvector-setup.md:309-316</path>
    </interface>
  </interfaces>
  <tests>
    <standards>
Manual Testing für Database Setup (kein pytest erforderlich für Story 1.2). Test-Framework: Direct Python Scripts in tests/ mit psycopg2.connect(). Test-Pattern: Setup → Execute → Verify → Cleanup. WICHTIG: WRITE-Tests sind Pflicht (nicht nur READ-Tests) - INSERT + DELETE Verification (Learning aus Story 1.1). Testing-Tools: psycopg2 für Connection-Tests, psql CLI für Schema-Verification, systemctl für Service-Status. Type Hints ERFORDERLICH in Test-Scripts (mypy strict mode).
    </standards>
    <locations>
      <location>tests/test_database.py - PostgreSQL Connection und Schema Validation Tests</location>
      <location>mcp_server/db/migrations/001_initial_schema.sql - SQL Migration Script (wird manuell getestet via psql)</location>
    </locations>
    <ideas>
      <test_idea ac="AC-1">
        <description>Service Status Test: systemctl status postgresql → "active (running)"</description>
        <approach>Bash command execution, verify exit code 0 und output enthält "active (running)"</approach>
      </test_idea>
      <test_idea ac="AC-1">
        <description>PostgreSQL Version Test: psql --version → 15+</description>
        <approach>Parse version string, assert major version >= 15</approach>
      </test_idea>
      <test_idea ac="AC-1">
        <description>pgvector Extension Test: SELECT * FROM pg_extension WHERE extname='vector'</description>
        <approach>SQL Query, assert rowcount == 1</approach>
      </test_idea>
      <test_idea ac="AC-2">
        <description>Schema Validation Test: \dt → 6 Tabellen sichtbar</description>
        <approach>SQL Query: SELECT count(*) FROM pg_tables WHERE schemaname='public' AND tablename IN (...), assert count == 6</approach>
      </test_idea>
      <test_idea ac="AC-3">
        <description>Index Validation Test: Verify 3 Indizes existieren (GIN, Session, LRU), IVFFlat NICHT gebaut</description>
        <approach>SQL Query: SELECT indexname FROM pg_indexes WHERE schemaname='public' AND indexname IN ('idx_l2_fts', 'idx_l0_session', 'idx_wm_lru'), assert count == 3</approach>
      </test_idea>
      <test_idea ac="AC-4">
        <description>Connection Test: psycopg2.connect() mit .env Credentials</description>
        <approach>Load .env.development, attempt connection, assert no exception, execute SELECT 1</approach>
      </test_idea>
      <test_idea ac="AC-4">
        <description>WRITE Test: INSERT INTO l0_raw + DELETE Verification</description>
        <approach>Generate UUID, INSERT test row, SELECT count (assert == 1), DELETE test row, SELECT count (assert == 0)</approach>
      </test_idea>
      <test_idea ac="AC-4">
        <description>Vector Operation Test: INSERT vector(1536) + Cosine Similarity Query</description>
        <approach>Create dummy vector [0.1]*1536, INSERT into l2_insights, Cosine Similarity Query (embedding &lt;=&gt; vector), verify result, DELETE test row</approach>
      </test_idea>
    </ideas>
  </tests>
</story-context>
