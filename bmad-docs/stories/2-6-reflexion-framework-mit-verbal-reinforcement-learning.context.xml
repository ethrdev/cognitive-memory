<story-context id="bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>2</epicId>
    <storyId>6</storyId>
    <title>Reflexion-Framework mit Verbal Reinforcement Learning</title>
    <status>drafted</status>
    <generatedAt>2025-11-16</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>bmad-docs/stories/2-6-reflexion-framework-mit-verbal-reinforcement-learning.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>MCP Server</asA>
    <iWant>bei schlechten Antworten (Reward &lt;0.3) Reflexionen via Haiku API generieren</iWant>
    <soThat>verbalisierte Lektionen in Episode Memory gespeichert werden</soThat>
    <tasks>
- Task 1: Implementiere generate_reflection() Method in HaikuClient (AC: 1, 2)
  - Subtask 1.1: Vervollständige generate_reflection() Stub in mcp_server/external/anthropic_client.py
  - Subtask 1.2: Implementiere strukturiertes Reflexion-Prompt
  - Subtask 1.3: Implementiere Parsing für Problem + Lesson Sections
  - Subtask 1.4: Wende @retry_with_backoff Decorator auf generate_reflection() an
  - Subtask 1.5: Extrahiere Token Count und berechne Cost
- Task 2: Integriere Reflexion-Trigger in Evaluation Pipeline (AC: 1, 3)
  - Subtask 2.1: Nach evaluate_answer(): Prüfe should_trigger_reflection()
  - Subtask 2.2: Implementiere Reflexion-to-Episode-Memory Flow
  - Subtask 2.3: Implementiere Reflexion Logging in PostgreSQL
- Task 3: Implementiere Episode Memory Retrieval für CoT Integration (AC: 4)
  - Subtask 3.1: Vor CoT Generation: Lade memory://episode-memory Resource
  - Subtask 3.2: Integriere Lessons Learned in CoT Reasoning
  - Subtask 3.3: Dokumentiere Episode Memory Integration Pattern
- Task 4: Testing und Validation (AC: alle)
  - Subtask 4.1: Manual Test mit Low-Quality Answer (Reward &lt;0.3 erwartet)
  - Subtask 4.2: Manual Test mit Medium-Quality Answer (Reward 0.3-0.7 erwartet)
  - Subtask 4.3: Manual Test mit ähnlicher Query nach Reflexion
  - Subtask 4.4: Validiere Episode Memory Retrieval
  - Subtask 4.5: Teste Retry-Logic mit simuliertem Rate Limit (optional)
</tasks>
  </story>

  <acceptanceCriteria>
AC-2.6.1: Haiku API Call für Reflexion - MCP Server ruft Haiku API korrekt auf
  - Trigger: should_trigger_reflection() returned True (Reward &lt;0.3)
  - Input: Query + Retrieved Context + Generated Answer + Evaluation Reasoning
  - Model: claude-3-5-haiku-20241022
  - Temperature: 0.7 (kreativ für Reflexion)
  - Max Tokens: 1000
  - Prompt: Strukturiertes Reflexion-Format

AC-2.6.2: Reflexion Format mit Problem + Lesson - Reflexion folgt definiertem Format
  - Problem: "Was lief schief?" (1-2 Sätze)
  - Lesson: "Was tun in Zukunft?" (1-2 Sätze)
  - Output: Verbalisierte Reflexion als String
  - Parsing: Extrahiere Problem und Lesson Sections

AC-2.6.3: Episode Memory Speicherung - Reflexion wird persistent gespeichert
  - MCP Tool: store_episode wird aufgerufen
  - Parameter: query (original), reward (from evaluation), reflection (Problem + Lesson)
  - Embedding: Query wird embedded via OpenAI API für Similarity-Suche
  - Speicherung: episode_memory Tabelle in PostgreSQL

AC-2.6.4: Abruf bei ähnlichen Queries - Lessons Learned sind nutzbar
  - Resource: memory://episode-memory vor CoT Generation abfragen
  - Similarity Threshold: Cosine Similarity &gt;0.70
  - Top-K: 3 ähnlichste Episodes
  - Integration: Lesson Learned in CoT Reasoning integrieren
</acceptanceCriteria>

  <artifacts>
    <docs>
      <artifact>
        <path>bmad-docs/tech-spec-epic-2.md</path>
        <title>Epic Technical Specification: RAG Pipeline &amp; Hybrid Calibration</title>
        <section>Story 2.6: Reflexion-Framework</section>
        <snippet>AC-2.6.1 bis AC-2.6.4 (authoritative) - Haiku API Call, Reflexion Format, Episode Memory Speicherung, Abruf bei ähnlichen Queries. Reflexion Generation API: generate_reflection(query, context, answer, evaluation_result) returns Problem + Lesson.</snippet>
      </artifact>
      <artifact>
        <path>bmad-docs/epics.md</path>
        <title>i-o - Epic Breakdown</title>
        <section>Story 2.6: Reflexion-Framework mit Verbal Reinforcement Learning</section>
        <snippet>Bei Reward &lt;0.3 triggert MCP Server Haiku API für verbalisierte Reflexion (Problem + Lesson Format). Reflexion wird in Episode Memory gespeichert und bei ähnlichen Queries (Similarity &gt;0.70) in CoT integriert.</snippet>
      </artifact>
      <artifact>
        <path>bmad-docs/architecture.md</path>
        <title>Cognitive Memory System v3.1.0-Hybrid - Architektur</title>
        <section>API-Integration - Anthropic Haiku API (Reflexion)</section>
        <snippet>Model: claude-3-5-haiku-20241022, Temperature: 0.7 (kreativ), Max Tokens: 1000. Usage: Verbal RL bei Reward &lt;0.3. Cost: ~€0.0015 per Reflexion (~€0.45/mo bei 300 Reflexionen).</snippet>
      </artifact>
      <artifact>
        <path>bmad-docs/architecture.md</path>
        <title>Cognitive Memory System v3.1.0-Hybrid - Architektur</title>
        <section>Error-Handling-Strategy - Fallback bei Haiku API Ausfall</section>
        <snippet>Retry-Logic: 4 Retries mit Exponential Backoff (1s, 2s, 4s, 8s). Fallback nach 4 Failed Retries: Skip Reflexion (degraded mode). System funktioniert ohne Reflexion, nur keine Lessons Learned gespeichert.</snippet>
      </artifact>
      <artifact>
        <path>bmad-docs/stories/2-5-self-evaluation-mit-haiku-api.md</path>
        <title>Story 2.5: Self-Evaluation mit Haiku API</title>
        <section>Completion Notes List - Haiku Client Infrastructure</section>
        <snippet>HaikuClient Class verfügbar in mcp_server/external/anthropic_client.py (lines 10-225). evaluate_answer() implementiert. generate_reflection() Stub vorhanden - muss in Story 2.6 implementiert werden.</snippet>
      </artifact>
      <artifact>
        <path>bmad-docs/stories/2-5-self-evaluation-mit-haiku-api.md</path>
        <title>Story 2.5: Self-Evaluation mit Haiku API</title>
        <section>Completion Notes List - Reflexion Utils</section>
        <snippet>mcp_server/utils/reflexion_utils.py (177 lines): should_trigger_reflection() Funktion verfügbar. get_reward_threshold() liest aus config.yaml (default: 0.3). Kann direkt genutzt werden.</snippet>
      </artifact>
    </docs>
    <code>
      <artifact>
        <path>mcp_server/external/anthropic_client.py</path>
        <kind>service</kind>
        <symbol>HaikuClient</symbol>
        <lines>10-225</lines>
        <reason>Enthält AsyncAnthropic Client, evaluate_answer() Method implementiert, generate_reflection() Stub muss vervollständigt werden</reason>
      </artifact>
      <artifact>
        <path>mcp_server/utils/reflexion_utils.py</path>
        <kind>utility</kind>
        <symbol>should_trigger_reflection()</symbol>
        <lines>1-177</lines>
        <reason>Trigger-Logic für Reflexion: Returns True wenn reward_score &lt; 0.3. Direkt nutzbar ohne Änderungen.</reason>
      </artifact>
      <artifact>
        <path>mcp_server/utils/retry_logic.py</path>
        <kind>utility</kind>
        <symbol>@retry_with_backoff</symbol>
        <lines>1-186</lines>
        <reason>Decorator für Exponential Backoff Retry-Logic. Muss auf generate_reflection() angewendet werden.</reason>
      </artifact>
      <artifact>
        <path>mcp_server/db/connection.py</path>
        <kind>database</kind>
        <symbol>PostgreSQL Connection Pool</symbol>
        <lines>unknown</lines>
        <reason>PostgreSQL Connection Pool für Episode Memory Speicherung. Bereits implementiert aus Story 1.2.</reason>
      </artifact>
      <artifact>
        <path>mcp_server/tools/store_episode.py</path>
        <kind>mcp-tool</kind>
        <symbol>store_episode</symbol>
        <lines>unknown</lines>
        <reason>MCP Tool für Episode Memory Speicherung. Parameter: query, reward, reflection. Bereits implementiert in Story 1.8.</reason>
      </artifact>
      <artifact>
        <path>mcp_server/resources/episode_memory.py</path>
        <kind>mcp-resource</kind>
        <symbol>memory://episode-memory</symbol>
        <lines>unknown</lines>
        <reason>MCP Resource für Episode Memory Retrieval. Query-Parameter: min_similarity (default: 0.70), top_k (default: 3). Bereits implementiert in Story 1.9.</reason>
      </artifact>
      <artifact>
        <path>config/config.yaml</path>
        <kind>configuration</kind>
        <symbol>base.memory.reflexion</symbol>
        <lines>unknown</lines>
        <reason>Reflexion Config bereits vorhanden aus Story 2.4: model (claude-3-5-haiku-20241022), temperature (0.7), max_tokens (1000), reward_threshold (0.3).</reason>
      </artifact>
      <artifact>
        <path>mcp_server/db/migrations/004_api_tracking_tables.sql</path>
        <kind>database-migration</kind>
        <symbol>api_cost_log table</symbol>
        <lines>unknown</lines>
        <reason>Cost-Tracking Infrastruktur für Haiku Reflexion API Calls. Columns: date, api_name, num_calls, token_count, estimated_cost. Nutze api_name='haiku_reflexion'.</reason>
      </artifact>
    </code>
    <dependencies>
      <python>
        <package name="anthropic" version="&gt;=0.30.0">Anthropic SDK für Haiku API Calls</package>
        <package name="openai" version="latest">OpenAI SDK für Query Embeddings (Episode Memory)</package>
        <package name="psycopg2" version="latest">PostgreSQL Adapter für Episode Memory Speicherung</package>
        <package name="pyyaml" version="^6.0">PyYAML für config.yaml Reading (aus Story 2.5)</package>
      </python>
    </dependencies>
  </artifacts>

  <constraints>
1. FILES ZU MODIFIZIEREN: Nur 1 File - mcp_server/external/anthropic_client.py (generate_reflection() Method implementieren)
2. FILES ZU NUTZEN (NO CHANGES): reflexion_utils.py, retry_logic.py, db/connection.py, config.yaml, store_episode.py, episode_memory.py
3. REFLEXION PROMPT: Strukturiertes Format mit "Problem:" und "Lesson:" Tags. Englisch (Best Practice für externe API Calls). Inklusive Examples für typische Fehlerarten.
4. TEMPERATURE: 0.7 für kreative Reflexion (nicht 0.0 wie bei Evaluation)
5. TRIGGER-THRESHOLD: Reward &lt;0.3 (aus should_trigger_reflection() Funktion)
6. EPISODE MEMORY RETRIEVAL: Similarity Threshold &gt;0.70, Top-K 3 (aus tech-spec FR009)
7. COST TRACKING: Log jeden Reflexion Call in api_cost_log mit api_name='haiku_reflexion'
8. FALLBACK STRATEGY: Bei 4 Failed Retries → Skip Reflexion (degraded mode), nicht kritisch für System-Funktionalität
9. PARSING ROBUSTNESS: Falls Problem/Lesson Parsing fehlschlägt, nutze gesamte Response als Lesson
10. COT INTEGRATION: Lessons erscheinen als separater Section in CoT Input: "Past experience from similar query suggests: {lesson}"
</constraints>

  <interfaces>
    <interface>
      <name>generate_reflection()</name>
      <kind>async method</kind>
      <signature>async def generate_reflection(query: str, context: List[str], answer: str, evaluation_result: Dict) -&gt; Dict[str, str]</signature>
      <path>mcp_server/external/anthropic_client.py</path>
      <description>Haiku API Call für Reflexion Generation. Input: query, context, answer, evaluation_result (mit reward_score, reasoning). Output: Dict mit problem, lesson, full_reflection. Temperature: 0.7, Max Tokens: 1000.</description>
    </interface>
    <interface>
      <name>should_trigger_reflection()</name>
      <kind>function</kind>
      <signature>def should_trigger_reflection(reward_score: float) -&gt; bool</signature>
      <path>mcp_server/utils/reflexion_utils.py</path>
      <description>Trigger-Check für Reflexion. Returns True wenn reward_score &lt; reward_threshold (default: 0.3 aus config.yaml). Bereits implementiert in Story 2.5.</description>
    </interface>
    <interface>
      <name>store_episode</name>
      <kind>MCP Tool</kind>
      <signature>store_episode(query: str, reward: float, reflection: str) -&gt; Dict</signature>
      <path>mcp_server/tools/store_episode.py</path>
      <description>MCP Tool für Episode Memory Speicherung. Embeddet Query via OpenAI API, speichert in episode_memory Tabelle. Bereits implementiert in Story 1.8.</description>
    </interface>
    <interface>
      <name>memory://episode-memory</name>
      <kind>MCP Resource</kind>
      <signature>memory://episode-memory?query={q}&amp;min_similarity={t}</signature>
      <path>mcp_server/resources/episode_memory.py</path>
      <description>MCP Resource für Episode Memory Retrieval. Query-Parameter: query (current query), min_similarity (default: 0.70), top_k (default: 3). Returns: [{query, reward, reflection, similarity}]. Bereits implementiert in Story 1.9.</description>
    </interface>
  </interfaces>

  <tests>
    <standards>
Manual Testing via Claude Code Interface (kein automated testing für Personal Use Projekt). Test-Ansatz: Story-specific Test-Cases definiert in Tasks/Subtasks. Validierung: End-to-End Flow Tests mit verschiedenen Reward Score Szenarien (Low &lt;0.3, Medium 0.3-0.7). Logging Validation: PostgreSQL Tabellen (episode_memory, api_cost_log) prüfen nach Tests.
</standards>
    <locations>
- tests/manual/ - Manual Test Scripts für Story-specific Testing
- mcp_server/db/migrations/ - Database Schema für Testing Validation
- PostgreSQL Tables: episode_memory, api_cost_log, api_retry_log
</locations>
    <ideas>
Test Idea 1 (AC-2.6.1, AC-2.6.2): Low-Quality Answer Test - Stelle Query ohne passenden Context, erwarte Reward &lt;0.3, verifiziere Reflexion getriggert, Problem + Lesson Sections korrekt geparst, Episode Memory enthält neue Reflexion.

Test Idea 2 (AC-2.6.1): Medium-Quality Answer Test - Stelle ambigue Query mit teilweise relevantem Context, erwarte Reward 0.3-0.7, verifiziere KEINE Reflexion getriggert, nur Evaluation Logging.

Test Idea 3 (AC-2.6.3, AC-2.6.4): Similar Query Test - First Query triggert Reflexion, Second Query ähnlich (Similarity &gt;0.70), verifiziere Episode Memory Resource liefert Lesson Learned, Lesson in CoT Reasoning integriert.

Test Idea 4 (AC-2.6.3): Episode Memory Validation - Nach 3-5 Reflexionen, prüfe episode_memory Tabelle: Reflexionen gespeichert, Query Embeddings korrekt, Similarity-Suche funktioniert (Top-3 bei &gt;0.70).

Test Idea 5 (AC-2.6.1): Retry-Logic Test (optional) - Mock 429 Response von Haiku API, verifiziere 4 Retries mit Exponential Backoff (~1s, 2s, 4s, 8s delays), Retry Count in api_retry_log geloggt.
</ideas>
  </tests>
</story-context>
