<story-context id="bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>1</epicId>
    <storyId>12</storyId>
    <title>IRR Validation & Contingency Plan (Enhancement E1)</title>
    <status>ready-for-dev</status>
    <generatedAt>2025-11-14</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>bmad-docs/stories/1-12-irr-validation-contingency-plan-enhancement-e1.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>Als Entwickler,</asA>
    <iWant>möchte ich Cohen's Kappa über alle Ground Truth Queries validieren,</iWant>
    <soThat>sodass ich sicherstelle dass IRR >0.70 ist (methodisch valide) und bei Bedarf Contingency Plan aktiviere.</soThat>
    <tasks>- [ ] Global Kappa Berechnung implementieren (AC: 1)
  - [ ] Lade alle Queries aus ground_truth Tabelle
  - [ ] Extrahiere judge1_score und judge2_score Arrays
  - [ ] Berechne Per-Query Kappa (bereits in Story 1.11 implementiert)
  - [ ] Berechne Macro-Average Kappa (Durchschnitt)
  - [ ] Berechne Micro-Average Kappa (alle Dokumente gepoolt)
  - [ ] Schreibe Ergebnisse in validation_results Tabelle

- [ ] Success Path Implementation (AC: 2)
  - [ ] IF Kappa ≥0.70: Print Success Message
  - [ ] Log Validation Result (timestamp, kappa_macro, kappa_micro, status="passed")
  - [ ] Mark Epic 1 als "ready for Epic 2" in Documentation
  - [ ] Update README.md mit Kappa-Wert

- [ ] Contingency Plan Module (AC: 3)
  - [ ] Identify High-Disagreement Queries (|score1 - score2| >0.4)
  - [ ] Sort by Disagreement Score (highest first)
  - [ ] Export zu CSV für Human Review

- [ ] Streamlit Human Tiebreaker UI (AC: 3.1)
  - [ ] Zeige Query + beide Judge Scores (GPT-4o vs. Haiku)
  - [ ] Zeige Top-5 Dokumente mit Scores
  - [ ] User wählt finale Relevanz-Entscheidung (Relevant/Nicht-Relevant)
  - [ ] Speichere in ground_truth.expected_docs (Override)
  - [ ] Progress Tracking: "12/24 Disagreements reviewed"

- [ ] Wilcoxon Signed-Rank Test (AC: 3.2)
  - [ ] scipy.stats.wilcoxon(judge1_scores, judge2_scores)
  - [ ] IF p-value <0.05: Systematischer Bias detected
  - [ ] Berechne Median Difference (z.B. GPT-4o systematisch +0.05 höher)
  - [ ] Recommend Threshold Adjustment (threshold = 0.5 + median_diff)
  - [ ] Log Recommendation in validation_results

- [ ] Judge Recalibration (AC: 3.3)
  - [ ] Extrahiere Low-Kappa Queries (Kappa <0.40)
  - [ ] Analysiere Prompt-Ineffizienz (fehlen explizite Kriterien?)
  - [ ] Update Dual Judge Prompts (von Story 1.11)
  - [ ] Re-run Dual Judge für Low-Kappa Queries
  - [ ] Re-calculate Global Kappa
  - [ ] IF noch Kappa <0.70: Manual Review aller Low-Kappa Queries

- [ ] Validation Results Persistence (Supporting)
  - [ ] CREATE TABLE validation_results (id, timestamp, kappa_macro, kappa_micro, status, contingency_actions, notes)
  - [ ] INSERT nach jedem Validation Run
  - [ ] Historische Tracking (z.B. Kappa vor vs. nach Contingency)

- [ ] Testing & Documentation (AC: alle)
  - [ ] Test: Mock 100 Queries mit Kappa 0.75 → Success Path
  - [ ] Test: Mock 100 Queries mit Kappa 0.65 → Contingency Path triggered
  - [ ] Test: Wilcoxon Test mit systematischem Bias (GPT-4o +0.1 höher)
  - [ ] Dokumentiere Contingency-Schritte in README.md
  - [ ] Dokumentiere Re-Validation Prozess</tasks>
  </story>

  <acceptanceCriteria>**Given** alle 50-100 Queries haben Dual Judge Scores
**When** ich die IRR Validation durchführe
**Then** wird Global Kappa berechnet:

1. **Kappa Aggregation:**
   - Aggregiere alle judge1_score vs judge2_score über alle Queries
   - Berechne Macro-Average Kappa (Durchschnitt aller Query-Kappas)
   - Berechne Micro-Average Kappa (alle Dokumente als einzelne Predictions)

2. **Success Path (Kappa ≥0.70):**
   - Success-Message: "IRR Validation Passed (Kappa: X.XX)"
   - Ground Truth ist ready für Hybrid Calibration (Epic 2)

3. **Contingency Path (Kappa <0.70):**
   - **Human Tiebreaker:**
     - Zeige Queries mit größter Judge-Disagreement (|score1 - score2| >0.4)
     - ethr entscheidet manuell (Streamlit UI)
     - Mindestens 20% der Queries mit Disagreement reviewen

   - **Wilcoxon Signed-Rank Test:**
     - Teste ob systematischer Bias zwischen Judges existiert
     - Falls ja: Kalibriere Threshold (z.B. GPT-4o threshold=0.55 statt 0.5)

   - **Judge Recalibration:**
     - Passe Prompts an (explizitere Relevanzkriterien)
     - Wiederhole Labeling für Low-Kappa Queries</acceptanceCriteria>

  <artifacts>
    <docs>
      <doc path="bmad-docs/PRD.md" title="Cognitive Memory System - Product Requirements Document (PRD)" section="Executive Summary & Description" snippet="Cognitive Memory System ist ein MCP-basiertes Gedächtnissystem, das Claude Code mit persistentem, kontextreichem Retrieval ausstattet. Key Improvement vs. v3.0.0-MCP: Methodisch valides Ground Truth durch echte unabhängige Dual Judges (GPT-4o + Haiku) statt pseudo-independence." />
      <doc path="bmad-docs/tech-spec-epic-1.md" title="Epic Technical Specification: MCP Server Foundation & Ground Truth Collection" section="Overview & Services" snippet="Epic 1 etabliert das technische und methodische Fundament für das Cognitive Memory System v3.1.0-Hybrid. Ziel: Funktionale MCP-basierte Persistence-Infrastruktur + statistisch robuste Evaluation-Baseline (Cohen's Kappa >0.70)." />
      <doc path="bmad-docs/epics.md" title="Epic 1: MCP Server Foundation & Ground Truth Collection" section="Story 1.12: IRR Validation & Contingency Plan" snippet="Story 1.12 implementiert IRR Validation mit Cohen's Kappa über alle Ground Truth Queries. Bei Kappa <0.70 wird Contingency Plan aktiviert: Human Tiebreaker, Wilcoxon Signed-Rank Test, Judge Recalibration." />
    </docs>
    <code>
      <artifact path="mcp_server/tools/dual_judge.py" kind="service" symbol="DualJudgeEvaluator" lines="28-519" reason="Existing dual judge implementation with Cohen's Kappa calculation - REUSE _calculate_cohen_kappa function" />
      <artifact path="mcp_server/tools/dual_judge.py" kind="function" symbol="_calculate_cohen_kappa" lines="230-274" reason="Core Cohen's Kappa calculation function - DO NOT RECREATE, import and reuse" />
      <artifact path="mcp_server/tools/dual_judge.py" kind="function" symbol="_manual_cohen_kappa" lines="276-315" reason="Manual Kappa calculation for edge cases - available as fallback" />
      <artifact path="mcp_server/db/connection.py" kind="service" symbol="get_connection" lines="96-151" reason="Database connection context manager pattern for PostgreSQL operations" />
      <artifact path="streamlit_apps/ground_truth_labeling.py" kind="ui" symbol="session_state_patterns" lines="382-406,418-506" reason="Streamlit UI patterns for session state management, progress tracking (st.progress, st.session_state), and button interactions - REUSE for Human Tiebreaker UI" />
      <artifact path="mcp_server/db/migrations/002_dual_judge_schema.sql" kind="schema" symbol="ground_truth_table" lines="20-21" reason="Database schema with judge1_score/judge2_score FLOAT[] columns and kappa field" />
    </code>
    <dependencies>
  <ecosystem name="python">
    <package name="scipy" version="^1.11.0" usage="Wilcoxon Signed-Rank Test for systematic bias detection" />
    <package name="scikit-learn" version="^1.3.0" usage="Cohen's Kappa calculation (cohen_kappa_score) - already used in dual_judge.py" />
    <package name="numpy" version="^1.24.0" usage="Array operations for score processing and statistical calculations" />
    <package name="streamlit" version="^1.28.0" usage="Human Tiebreaker UI for manual review of high-disagreement queries" />
    <package name="psycopg2-binary" version="^2.9.0" usage="PostgreSQL database connection for validation_results persistence" />
    <package name="openai" version="^1.0.0" usage="GPT-4o API client (already used in dual_judge.py)" />
    <package name="anthropic" version="^0.25.0" usage="Haiku API client (already used in dual_judge.py)" />
  </ecosystem>
  <ecosystem name="postgresql">
    <package name="postgresql" version="15+" usage="Database server with pgvector extension for ground_truth storage" />
    <package name="pgvector" version="^0.2.0" usage="Vector extension for semantic search (already configured)" />
  </ecosystem>
</dependencies>
  </artifacts>

  <constraints>
    <constraint type="database" description="Use synchronous database connection with context manager pattern: with get_connection() as conn:" />
    <constraint type="kappa-calculation" description="Reuse existing Cohen's Kappa calculation from dual_judge.py - DO NOT RECREATE function" />
    <constraint type="api-rate-limits" description="Implement retry logic with exponential backoff for API rate limits (delays: [1, 2, 4, 8] seconds)" />
    <constraint type="score-validation" description="Validate score ranges (0.0-1.0) and handle NaN/Inf edge cases in Kappa calculations" />
    <constraint type="transaction-management" description="Use explicit conn.commit() after INSERT/UPDATE/DELETE with try/except rollback on error" />
    <constraint type="contingency-thresholds" description="Kappa target >=0.70, disagreement threshold >0.4, low-kappa threshold <0.40" />
    <constraint type="test-coverage" description="Test success path (Kappa >=0.70) and contingency path (Kappa <0.70) with mock data" />
  </constraints>
  <interfaces>
    <interface name="Database Connection Pool" kind="context_manager" signature="with get_connection() as conn:" path="mcp_server/db/connection.py:96-151" />
    <interface name="Cohen's Kappa Calculator" kind="function" signature="def _calculate_cohen_kappa(judge1_scores: list[float], judge2_scores: list[float]) -> float" path="mcp_server/tools/dual_judge.py:230-274" />
    <interface name="Ground Truth Table Schema" kind="database_table" signature="ground_truth(id, query, expected_docs, judge1_score FLOAT[], judge2_score FLOAT[], kappa FLOAT)" path="mcp_server/db/migrations/002_dual_judge_schema.sql:20-21" />
    <interface name="Wilcoxon Signed-Rank Test" kind="function" signature="scipy.stats.wilcoxon(judge1_scores, judge2_scores)" path="external:scipy" />
    <interface name="Streamlit Session State" kind="ui_pattern" signature="st.session_state, st.progress(), st.button(), progress tracking patterns" path="streamlit_apps/ground_truth_labeling.py:382-406,418-506" />
  </interfaces>
  <tests>
    <standards>Unit tests use pytest with asyncio support and mock clients for API testing. Integration tests use real database connections. Test patterns include fixture-based setup, async/await testing, and pytest.approx for float comparisons. Mock data generation for scenarios like systematic bias (+0.1 difference) and various Kappa levels.</standards>
    <locations>tests/test_dual_judge.py (existing patterns), tests/test_irr_validation.py (new file for Story 1.12), tests/ directory for all unit tests, integration tests with real PostgreSQL database</locations>
    <ideas>
      <test idea="Macro-Average Kappa Calculation" acceptance_criteria="1" description="Mock 5 queries mit bekannten Kappas [0.80, 0.75, 0.65, 0.70, 0.85] → expected 0.75" />
      <test idea="Micro-Average Kappa Calculation" acceptance_criteria="1" description="Pool 8 documents from all queries, compare with sklearn cohen_kappa_score" />
      <test idea="Success Path (Kappa ≥0.70)" acceptance_criteria="2" description="Mock 100 queries mit Kappa 0.75 → status='passed', no contingency triggered" />
      <test idea="Contingency Path (Kappa <0.70)" acceptance_criteria="3" description="Mock 100 queries mit Kappa 0.65 → status='contingency_triggered', high_disagreement_queries identified" />
      <test idea="Wilcoxon Systematic Bias Detection" acceptance_criteria="3.2" description="GPT-4o systematisch +0.1 höher → p-value <0.05, threshold adjustment recommended" />
      <test idea="End-to-End Validation with Database" acceptance_criteria="all" description="Seed 50 ground truth queries, run validation, check validation_results table" />
    </ideas>
  </tests>
</story-context>
