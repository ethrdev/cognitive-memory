<story-context id="bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>3</epicId>
    <storyId>5</storyId>
    <title>Latency Benchmarking &amp; Performance Optimization</title>
    <status>drafted</status>
    <generatedAt>2025-11-18</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>bmad-docs/stories/3-5-latency-benchmarking-performance-optimization.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>Entwickler</asA>
    <iWant>End-to-End Latency systematisch messen und optimieren</iWant>
    <soThat>NFR001 (Query Response Time &lt;5s p95) garantiert erfüllt ist</soThat>
    <tasks>
### Task 1: Create Latency Benchmarking Script (AC: 3.5.1)
- Subtask 1.1: Create `mcp_server/benchmarking/latency_benchmark.py` script
- Subtask 1.2: Load 100 test queries from Golden Test Set (stratified: 40 Short, 40 Medium, 20 Long)
- Subtask 1.3: Implement high-precision timing with `time.perf_counter()` for each component
- Subtask 1.4: Calculate End-to-End Latency (sum of all components)
- Subtask 1.5: Store measurements in JSON

### Task 2: Statistical Analysis und Percentile Calculation (AC: 3.5.2)
- Subtask 2.1: Import measurements from JSON
- Subtask 2.2: Calculate percentiles: p50, p95, p99 für End-to-End Latency
- Subtask 2.3: Calculate percentiles für each component
- Subtask 2.4: Validate gegen NFR001: p95 &lt;5s (Pass/Fail)
- Subtask 2.5: Validate component thresholds

### Task 3: Performance Optimization (AC: 3.5.3)
- Subtask 3.1: Identify bottleneck component
- Subtask 3.2-3.4: Optimize specific components if thresholds not met
- Subtask 3.5: Re-run benchmark after optimizations
- Subtask 3.6: Validate NFR001 compliance after optimizations

### Task 4: Documentation und Baseline Establishment (AC: 3.5.4)
- Subtask 4.1-4.6: Create comprehensive performance documentation

### Task 5: Testing and Validation (All ACs)
- Subtask 5.1-5.6: Manual validation of all components
    </tasks>
  </story>

  <acceptanceCriteria>
### AC-3.5.1: Latency Measurement Infrastructure
- Query Mix: 40 Short, 40 Medium, 20 Long (stratified)
- Measured Metrics: End-to-End Latency with component breakdown
- Components: Query Expansion, Embedding, Hybrid Search, CoT Generation, Evaluation
- Percentiles: p50, p95, p99
- Tool: Python Script mit time.perf_counter()
- Output: JSON file mit measurements

### AC-3.5.2: Performance Goal Validation
- p95 End-to-End Latency: &lt;5s (NFR001)
- p95 Retrieval Time: &lt;1s (Hybrid Search)
- p50 End-to-End Latency: &lt;3s
- Document results in /docs/performance-benchmarks.md

### AC-3.5.3: Performance Optimization (Falls Ziele nicht erfüllt)
- Falls Hybrid Search &gt;1s p95: Optimize pgvector Index
- Falls CoT Generation &gt;3s p95: Kürze Context, optimize Prompt
- Falls Evaluation &gt;1s p95: Check Haiku API Latency, erwäge Batch

### AC-3.5.4: Performance Documentation und Baseline
- Dokumentation: /docs/performance-benchmarks.md
- Sections: Setup, Results, NFR001 Compliance, Baseline, Recommendations
  </acceptanceCriteria>

  <artifacts>
    <docs>
<!-- Epic 3 Documentation -->
<doc>
  <path>bmad-docs/epics.md</path>
  <title>Epic Breakdown - Epic 3</title>
  <section>Story 3.5: Latency Benchmarking &amp; Performance Optimization</section>
  <snippet>Story 3.5 implements systematic latency measurement and optimization to validate NFR001 (Query Response Time &lt;5s p95). Benchmarks 100 test queries with component-level breakdown to identify bottlenecks and enable data-driven optimization.</snippet>
</doc>

<doc>
  <path>bmad-docs/tech-spec-epic-3.md</path>
  <title>Technical Specification - Epic 3</title>
  <section>Story 3.5: Latency Benchmarking</section>
  <snippet>Comprehensive performance validation with 100 test queries (stratified mix), component breakdown (Query Expansion, Embedding, Hybrid Search, CoT, Evaluation), and percentile calculations (p50, p95, p99) to validate NFR001 compliance.</snippet>
</doc>

<doc>
  <path>bmad-docs/architecture.md</path>
  <title>Architecture Documentation</title>
  <section>NFR001: Latency Targets</section>
  <snippet>End-to-End Query Response &lt;5s (p95), Hybrid Search &lt;1s (p95), CoT Generation ~2-3s (median), Haiku Evaluation ~0.5s (median), OpenAI Embeddings ~200-300ms (median). Latency targets are critical for user experience in philosophical conversations.</snippet>
</doc>

<doc>
  <path>bmad-docs/architecture.md</path>
  <title>Architecture Documentation</title>
  <section>Performance Architecture</section>
  <snippet>pgvector IVFFlat Index with lists=100 (optimized for &lt;100k vectors). Alternative: HNSW (faster, more memory). Query optimization: Top-5 limit reduces context window risk. Connection pooling optional for performance issues.</snippet>
</doc>

<doc>
  <path>bmad-docs/architecture.md</path>
  <title>Architecture Documentation</title>
  <section>Database Performance</section>
  <snippet>pgvector IVFFlat Index rebuild at &gt;10k new L2 Insights. RRF Fusion parallel semantic + keyword search. Connection pooling (psycopg2.pool) optional for performance issues.</snippet>
</doc>

<!-- Previous Story Context -->
<doc>
  <path>bmad-docs/stories/3-4-claude-code-fallback-fuer-haiku-api-ausfall-degraded-mode.md</path>
  <title>Story 3.4 - Fallback Implementation</title>
  <section>Performance Characteristics</section>
  <snippet>Haiku API Latency: ~0.5-1s (external API call). Claude Code Fallback: ~1-2s (heuristic eval). Retry-Logic worst-case: 15s (4 retries with exponential backoff). Fallback-state check: ~1ms overhead. Health check background task: no direct query impact.</snippet>
</doc>
    </docs>

    <code>
<!-- Existing Code to REUSE -->
<artifact>
  <path>mcp_server/external/openai_client.py</path>
  <kind>service</kind>
  <symbol>create_embedding()</symbol>
  <lines>unknown</lines>
  <reason>REUSE for measuring Embedding Time component in benchmarking. OpenAI text-embedding-3-small API integration.</reason>
</artifact>

<artifact>
  <path>mcp_server/external/anthropic_client.py</path>
  <kind>service</kind>
  <symbol>evaluate_answer(), evaluate_answer_with_fallback()</symbol>
  <lines>unknown</lines>
  <reason>REUSE for measuring Evaluation Time component. Story 3.4 added fallback wrapper - measure wrapper overhead (~1-5ms).</reason>
</artifact>

<artifact>
  <path>mcp_server/tools/hybrid_search.py</path>
  <kind>tool</kind>
  <symbol>hybrid_search()</symbol>
  <lines>unknown</lines>
  <reason>REUSE for measuring Hybrid Search Time component. pgvector semantic search + PostgreSQL full-text search with RRF fusion.</reason>
</artifact>

<artifact>
  <path>mcp_server/db/connection.py</path>
  <kind>database</kind>
  <symbol>PostgreSQL connection pool</symbol>
  <lines>unknown</lines>
  <reason>REUSE for database connectivity in benchmarking script.</reason>
</artifact>

<artifact>
  <path>mcp_server/state/fallback_state.py</path>
  <kind>service</kind>
  <symbol>is_fallback_active()</symbol>
  <lines>unknown</lines>
  <reason>NEW from Story 3.4. Measure fallback-state check overhead (expected: &lt;1ms).</reason>
</artifact>

<artifact>
  <path>mcp_server/utils/retry_logic.py</path>
  <kind>utility</kind>
  <symbol>@retry_with_backoff decorator</symbol>
  <lines>unknown</lines>
  <reason>From Story 3.3. Applied to evaluate_answer(). Measure retry overhead when API failures occur (worst-case: +15s for 4 retries).</reason>
</artifact>

<!-- Golden Test Set Data -->
<artifact>
  <path>bmad-docs/golden-test-set.json</path>
  <kind>data</kind>
  <symbol>Golden Test Set</symbol>
  <lines>N/A</lines>
  <reason>100 stratified test queries (40 Short, 40 Medium, 20 Long) from Story 3.1. Primary input for latency benchmarking.</reason>
</artifact>

<!-- Files Created in Story 3.4 (Relevant for Measurement) -->
<artifact>
  <path>mcp_server/health/haiku_health_check.py</path>
  <kind>service</kind>
  <symbol>periodic_health_check()</symbol>
  <lines>unknown</lines>
  <reason>NEW from Story 3.4. Background task (15-min intervals) - no direct query pipeline impact, but good to verify isolation.</reason>
</artifact>

<artifact>
  <path>mcp_server/utils/fallback_logger.py</path>
  <kind>utility</kind>
  <symbol>log_fallback_activation()</symbol>
  <lines>unknown</lines>
  <reason>NEW from Story 3.4. Database writes for fallback status (~10-20ms) - only during fallback activation, not normal queries.</reason>
</artifact>
    </code>

    <dependencies>
<python>
  <package name="time" version="standard library">High-precision timing with time.perf_counter()</package>
  <package name="json" version="standard library">Store benchmark measurements</package>
  <package name="statistics" version="standard library">Calculate percentiles (p50, p95, p99)</package>
  <package name="psycopg2" version="unknown">PostgreSQL database connectivity</package>
  <package name="openai" version="unknown">Embeddings API integration</package>
  <package name="anthropic" version="unknown">Haiku API integration</package>
  <package name="numpy" version="unknown">Statistical analysis (optional)</package>
</python>
    </dependencies>
  </artifacts>

  <constraints>
<!-- Performance Constraints -->
<constraint>NFR001: End-to-End Query Response Time must be &lt;5s (p95) - Critical for user experience</constraint>
<constraint>Component thresholds: Hybrid Search &lt;1s p95, CoT Generation ~2-3s median, Evaluation &lt;1s p95</constraint>
<constraint>Benchmarking must use Golden Test Set (100 queries) for consistency with Model Drift Detection (Story 3.2)</constraint>
<constraint>Component-level timing required (not just end-to-end) to enable targeted optimization</constraint>
<constraint>Use time.perf_counter() for high-precision timing (nanosecond resolution)</constraint>

<!-- Integration Constraints -->
<constraint>Measure actual retry frequency from Story 3.3 retry-logic (expected: &lt;1% of queries trigger retries)</constraint>
<constraint>Measure fallback-state check overhead from Story 3.4 (expected: ~1-5ms)</constraint>
<constraint>Background tasks (health check) must not impact query latency</constraint>

<!-- Optimization Constraints -->
<constraint>Optimize only components that exceed thresholds (data-driven approach)</constraint>
<constraint>pgvector Index optimization: IVFFlat lists parameter (100, 200, 500) or consider HNSW</constraint>
<constraint>If CoT Generation slow: reduce Retrieved Context (Top-3 instead of Top-5) or shorten prompt</constraint>
<constraint>If Evaluation slow: check Haiku API latency vs. network, consider batch evaluation</constraint>

<!-- Documentation Constraints -->
<constraint>Document all results in /docs/performance-benchmarks.md (Deutsch)</constraint>
<constraint>Establish baseline for future regression tests (store current p95 values)</constraint>
<constraint>Include NFR001 compliance status (Pass/Fail with evidence)</constraint>
  </constraints>

  <interfaces>
<!-- Benchmarking Script Interface -->
<interface>
  <name>benchmark_query(query, context)</name>
  <kind>function</kind>
  <signature>def benchmark_query(query: str, context: list[str]) -> dict[str, float]</signature>
  <path>mcp_server/benchmarking/latency_benchmark.py</path>
  <description>Runs single query with component-level timing breakdown. Returns dict with keys: query_expansion, embedding, hybrid_search, cot_generation, evaluation, total.</description>
</interface>

<interface>
  <name>run_benchmark(test_queries)</name>
  <kind>function</kind>
  <signature>def run_benchmark(test_queries: list[dict]) -> list[dict]</signature>
  <path>mcp_server/benchmarking/latency_benchmark.py</path>
  <description>Executes benchmark on all test queries. Returns list of measurement dicts with query_id, breakdown, total_latency.</description>
</interface>

<interface>
  <name>calculate_percentiles(measurements)</name>
  <kind>function</kind>
  <signature>def calculate_percentiles(measurements: list[dict]) -> dict[str, dict[str, float]]</signature>
  <path>mcp_server/benchmarking/latency_benchmark.py</path>
  <description>Calculates p50, p95, p99 for each component and total latency. Returns nested dict: {component: {p50, p95, p99}}.</description>
</interface>

<interface>
  <name>generate_report(percentiles, nfr_compliance)</name>
  <kind>function</kind>
  <signature>def generate_report(percentiles: dict, nfr_compliance: bool) -> str</signature>
  <path>mcp_server/benchmarking/latency_benchmark.py</path>
  <description>Generates performance-benchmarks.md documentation with benchmark setup, results table, NFR001 compliance, and recommendations.</description>
</interface>

<!-- External APIs to Measure -->
<interface>
  <name>OpenAI Embeddings API</name>
  <kind>REST API</kind>
  <signature>POST /v1/embeddings {model, input}</signature>
  <path>External - openai_client.py wrapper</path>
  <description>Measure embedding time component. Expected: ~0.2-0.5s per call.</description>
</interface>

<interface>
  <name>Anthropic Haiku API</name>
  <kind>REST API</kind>
  <signature>POST /v1/messages {model, messages, max_tokens, temperature}</signature>
  <path>External - anthropic_client.py wrapper</path>
  <description>Measure evaluation time component. Expected: ~0.5-1s per call. Includes retry-logic overhead from Story 3.3.</description>
</interface>

<!-- PostgreSQL/pgvector Interface -->
<interface>
  <name>hybrid_search tool</name>
  <kind>MCP tool</kind>
  <signature>hybrid_search(query_embedding, query_text, top_k, weights)</signature>
  <path>mcp_server/tools/hybrid_search.py</path>
  <description>Measure hybrid search time component. Expected: &lt;1s p95. Uses pgvector IVFFlat index (lists=100) + PostgreSQL full-text search with RRF fusion.</description>
</interface>
  </interfaces>

  <tests>
    <standards>
Manual testing approach (consistent with Story 3.2 and 3.4). No automated unit tests for personal project. Focus on:
1. Benchmark execution on 100 Golden Test queries
2. Percentile calculation validation (manual spot-check)
3. Component breakdown verification (sum to total)
4. NFR001 threshold validation
5. Documentation completeness review
    </standards>

    <locations>
mcp_server/benchmarking/latency_benchmark.py (new script)
bmad-docs/golden-test-set.json (input data)
docs/performance-benchmarks.md (output documentation)
    </locations>

    <ideas>
<!-- Test Ideas Mapped to Acceptance Criteria -->

**AC-3.5.1: Latency Measurement Infrastructure**
- Test 1.1: Verify benchmark script loads 100 Golden Test queries correctly (stratified: 40/40/20)
- Test 1.2: Verify time.perf_counter() timing precision (nanosecond resolution)
- Test 1.3: Verify component breakdown captured for all 5 components (Query Expansion, Embedding, Hybrid Search, CoT, Evaluation)
- Test 1.4: Verify JSON output format correct (timestamp, query_id, breakdown dict, total_latency)
- Test 1.5: Verify component times sum to total latency (±5ms tolerance for timing precision)

**AC-3.5.2: Performance Goal Validation**
- Test 2.1: Calculate percentiles and verify p95 &lt;5s for End-to-End (NFR001)
- Test 2.2: Verify p95 &lt;1s for Hybrid Search component
- Test 2.3: Verify p50 &lt;3s for End-to-End latency
- Test 2.4: Verify percentile calculation accuracy (manual spot-check against sorted data)
- Test 2.5: Verify NFR001 Pass/Fail threshold logic correct

**AC-3.5.3: Performance Optimization**
- Test 3.1: If Hybrid Search &gt;1s p95, test pgvector Index optimization (vary lists parameter: 100, 200, 500)
- Test 3.2: If CoT Generation &gt;3s p95, test reducing Retrieved Context (Top-3 instead of Top-5)
- Test 3.3: If Evaluation &gt;1s p95, measure Haiku API latency vs. network latency
- Test 3.4: Simulate retry-logic trigger (API 429 Rate Limit) → measure retry overhead
- Test 3.5: Re-run benchmark after optimizations → verify p95 improvements

**AC-3.5.4: Performance Documentation**
- Test 4.1: Verify performance-benchmarks.md created with all required sections
- Test 4.2: Verify benchmark setup documented (100 queries, stratified mix, timing methodology)
- Test 4.3: Verify results table complete (percentiles for each component + total)
- Test 4.4: Verify NFR001 compliance status clear (✅ Pass or ❌ Fail with evidence)
- Test 4.5: Verify baseline values stored for future regression tests
- Test 4.6: Verify optimization recommendations included (if optimizations applied)

**Edge Cases to Test:**
- Retry-Logic Impact: Simulate Haiku API failure → measure evaluation time with 1-4 retries
- Fallback-State Overhead: Measure is_fallback_active() call latency (expected: &lt;1ms)
- API Latency Variance: Compare p95-p50 delta for Embedding and Evaluation (acceptable if &lt;0.5s)
- Background Task Isolation: Verify health check task doesn't impact query latency
    </ideas>
  </tests>
</story-context>
