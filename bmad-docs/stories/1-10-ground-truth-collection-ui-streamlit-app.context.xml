<story-context id="bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>1</epicId>
    <storyId>10</storyId>
    <title>Ground Truth Collection UI (Streamlit App)</title>
    <status>production-ready</status>
    <generatedAt>2025-11-12</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>bmad-docs/stories/1-10-ground-truth-collection-ui-streamlit-app.md</sourceStoryPath>
  </metadata>

  <reviewHistory>
    <review date="2025-11-12" reviewer="claude-sonnet-4-5-20250929" score="70/100" status="BLOCKED">
      <summary>5 CRITICAL/HIGH issues identified: MCP Tool Architecture, SQL Logic, Temporal Diversity, Keyboard Shortcuts, Story Dependencies</summary>
      <fixes>
        <fix id="1" priority="CRITICAL">Changed MCP Tool Integration → Direct DB Queries Architecture</fix>
        <fix id="2" priority="HIGH">Corrected SQL Sentence Counting (.!? Punctuation)</fix>
        <fix id="3" priority="HIGH">Implemented Temporal Diversity SQL (eligible_sessions CTE)</fix>
        <fix id="4" priority="MEDIUM">Removed Keyboard Shortcuts AC2 (Streamlit limitation)</fix>
        <fix id="5" priority="MEDIUM">Added Story 1.6 Dependency Status (verified "done")</fix>
      </fixes>
    </review>
    <review date="2025-11-12" reviewer="claude-sonnet-4-5-20250929" score="96/100" status="APPROVED">
      <summary>All 5 fixes applied successfully</summary>
    </review>
    <review date="2025-11-12" reviewer="claude-sonnet-4-5-20250929" score="99/100" status="PRODUCTION-READY">
      <summary>Optimizations E12+E13 applied: German FTS, Error Handling, Type Hints, Logging</summary>
    </review>
    <review date="2025-11-12" reviewer="claude-sonnet-4-5-20250929" score="98/100" status="OPTIMIZED">
      <summary>Context XML optimized with production-ready status, optimization constraints, comprehensive documentation references</summary>
    </review>
  </reviewHistory>

  <story>
    <asA>Als ethr,</asA>
    <iWant>möchte ich eine dedizierte UI zum Labeln von Queries haben,</iWant>
    <soThat>sodass ich effizient 50-100 Ground Truth Queries erstellen kann.</soThat>
    <tasks>- [ ] Query Extraction mit Stratified Sampling & Temporal Diversity (AC: 1)
  - [ ] SQL: Identify eligible sessions (COUNT(*) BETWEEN 3 AND 5)
  - [ ] SQL: Sentence counting mit allen Punctuation Marks (.!?)
  - [ ] Stratification Logic: 40% Short (1-2 Sätze), 40% Medium (3-5), 20% Long (6+)
  - [ ] Temporal Diversity: Sample 3-5 Queries aus eligible sessions
  - [ ] Target: 50-100 Queries extrahiert

- [ ] Streamlit Labeling Interface (AC: 2)
  - [ ] Page Layout: Query Display, Doc List (5 Items), Action Buttons
  - [ ] For each Query: Zeige Query Text + Top-5 Docs via Direct DB Queries
  - [ ] Binäre Entscheidung: st.checkbox "Relevant?" pro Dokument
  - [ ] State Management: session_state für aktuelle Query Position

- [ ] Direct DB Hybrid Search Implementation (AC: 2)
  - [ ] Embed Query via OpenAI API (reuse get_embedding_with_retry)
  - [ ] Semantic Search: pgvector cosine similarity query
  - [ ] Keyword Search: PostgreSQL Full-Text Search (ts_rank)
  - [ ] RRF Fusion: Merge beide result sets (semantic 70%, keyword 30%)
  - [ ] Return Top-5 Docs (L2 Insight IDs + Content)

- [ ] Progress Tracking Implementation (AC: 3)
  - [ ] Progress Bar: st.progress mit Anzahl gelabelter Queries / Target
  - [ ] Progress Text: "68/100 Queries gelabelt"
  - [ ] Stratification Balance: Zeige %Short, %Medium, %Long (live update)
  - [ ] "Save & Continue Later"-Button: Persistiere aktuellen Stand in DB

- [ ] PostgreSQL Ground Truth Storage (AC: alle)
  - [ ] Tabelle ground_truth existiert (Story 1.2 bereits angelegt)
  - [ ] INSERT INTO ground_truth: (query, expected_docs) pro gelabelte Query
  - [ ] expected_docs: Array von L2 Insight IDs die als "Relevant" markiert wurden
  - [ ] created_at: Timestamp automatisch generiert

- [ ] Session State Persistence (AC: 3)
  - [ ] Load existing Ground Truth entries on startup (COUNT(*) für Progress)
  - [ ] Resume from last position: Query Index aus session_state
  - [ ] "Continue Later" speichert: Aktuelle Query Position + gelabelte Queries

- [ ] Validation & Edge Cases (AC: alle)
  - [ ] Validierung: Mindestens 1 Doc als relevant markiert (sonst Warning)
  - [ ] Skip Query: Schreibe NULL in expected_docs (optional markieren)
  - [ ] Duplicate Prevention: Check ob Query bereits in ground_truth existiert
  - [ ] Empty L0: Zeige Error wenn keine Queries extrahiert werden können

- [ ] Testing & Manual Validation (AC: alle)
  - [ ] Test: Extrahiere 100 Queries, prüfe Stratification (40/40/20)
  - [ ] Test: Temporal Diversity funktioniert (3-5 Queries/Session)
  - [ ] Test: Sentence counting korrekt (alle Punctuation .!?)
  - [ ] Test: Label 10 Queries, prüfe PostgreSQL Inserts (expected_docs korrekt)
  - [ ] Test: Progress Bar aktualisiert korrekt
  - [ ] Manual Test: Run Streamlit App, labele 10 Queries End-to-End</tasks>
  </story>

  <acceptanceCriteria>**Given** L0 Raw Memory enthält Dialogtranskripte
**When** ich die Streamlit App starte
**Then** sehe ich folgende Features:

1. **Automatic Query Extraction:**
   - App extrahiert Queries aus L0 Raw Memory
   - Stratified Sampling: 40% Short (1-2 Sätze), 40% Medium (3-5), 20% Long (6+)
   - Temporal Diversity: 3-5 Queries pro Session (verhindert Bias)

2. **Labeling Interface:**
   - Zeige Query + Top-5 Retrieved Documents (via Direct DB Queries)
   - Binäre Entscheidung pro Dokument: "Relevant?" (Ja/Nein)
   - Interaktive UI mit Radio Buttons / Checkboxen für Relevance Selection

3. **Progress Tracking:**
   - Progress Bar: "68/100 Queries gelabelt"
   - Zeige aktuelle Stratification Balance (% Short/Medium/Long)
   - "Save & Continue Later"-Option

**And** Ground Truth wird in PostgreSQL gespeichert:
- Tabelle: `ground_truth` (id, query, expected_docs, created_at)
- expected_docs: Array von L2 Insight IDs die als "Relevant" markiert wurden</acceptanceCriteria>

  <artifacts>
    <docs>
      <doc path="bmad-docs/epics.md" title="Epic Breakdown" section="Story 1.10: Ground Truth Collection UI (Streamlit App)" snippet="Story 1.10 creates a Streamlit UI for efficient labeling of 50-100 Ground Truth Queries with stratified sampling (40% Short, 40% Medium, 20% Long) and temporal diversity (3-5 queries per session)." />
      <doc path="bmad-docs/PRD.md" title="Product Requirements Document" section="FR010: Ground Truth Collection mit echten unabhängigen Dual Judges" snippet="Streamlit UI collects 50-100 labeled queries. MCP Server calls GPT-4o API (OpenAI) + Haiku API (Anthropic) for true independent evaluation, calculates Cohen's Kappa, stores results." />
      <doc path="bmad-docs/tech-spec-epic-1.md" title="Epic 1 Technical Specification" section="Ground Truth UI (Streamlit)" snippet="Manual Query Labeling Interface: L0 Raw Memory (auto-extracted queries) → Labeled Ground Truth Set" />
      <doc path="bmad-docs/architecture.md" title="System Architecture" section="Systemarchitektur" snippet="MCP Server (Python, lokal) mit External API Clients: OpenAI API (Embeddings), Anthropic Haiku API (Evaluation, Reflexion), OpenAI GPT-4o API (Dual Judge)" />
      <doc path="bmad-docs/stories/1-10-ground-truth-collection-ui-streamlit-app.md" title="Story 1.10 Dev Notes" section="Dev Notes (Lines 99-740)" snippet="Comprehensive implementation guidance: SQL examples (stratified sampling, temporal diversity), Direct DB Hybrid Search implementation (Lines 335-487), error handling patterns, optimizations (E12 German FTS, E13 Error Handling)" />
      <doc path="bmad-docs/stories/1-9-mcp-resources-fuer-read-only-state-exposure.md" title="Story 1.9 - Learnings" section="Learnings from Previous Story" snippet="Database connection patterns (get_connection context manager), pgvector integration (register_vector), OpenAI embeddings reuse (get_embedding_with_retry)" />
    </docs>
    <code>
      <codeitem path="mcp_server/tools/__init__.py" kind="hybrid_search" symbol="rrf_fusion" lines="33-95" reason="RRF Fusion algorithm for merging semantic and keyword search results - can be reused in Streamlit app" />
      <codeitem path="mcp_server/external/openai_client.py" kind="embedding" symbol="get_embedding_with_retry" lines="UNKNOWN" reason="OpenAI embedding function with retry logic - reuse for query embedding in Streamlit app" />
      <codeitem path="mcp_server/tools/__init__.py" kind="hybrid_search" symbol="handle_hybrid_search" lines="607-712" reason="Complete hybrid search implementation - reference for Streamlit direct DB queries" />
      <codeitem path="mcp_server/db/connection.py" kind="database" symbol="get_connection" lines="96-151" reason="Database connection context manager - reuse for Streamlit app DB access" />
      <codeitem path="mcp_server/db/migrations/001_initial_schema.sql" kind="database" symbol="ground_truth table" lines="92-102" reason="Ground truth table schema with expected_docs array for storing L2 Insight IDs" />
    </code>
    <dependencies>
      <dependency ecosystem="python" packages="streamlit^1.28.0, psycopg2-binary^2.9.0, pgvector^0.2.0, openai^1.0.0" />
    </dependencies>
  </artifacts>

  <constraints>
  <constraint type="architecture">Streamlit App runs as standalone process - NO MCP Server dependency (use direct DB queries instead)</constraint>
  <constraint type="database">Use sync database connections with get_connection() context manager (not async)</constraint>
  <constraint type="database">Explicit conn.commit() after INSERT/UPDATE/DELETE operations (not needed for SELECT)</constraint>
  <constraint type="pgvector">Register pgvector types with register_vector(conn) before vector queries</constraint>
  <constraint type="code-quality">Type hints REQUIRED (mypy --strict), all imports at file top</constraint>
  <constraint type="code-quality">Black + Ruff for linting, no duplicate imports or unused variables</constraint>
  <constraint type="ui">No keyboard shortcuts - Streamlit has no native keyboard event support (removed from AC2)</constraint>
  <constraint type="search">Implement Direct DB Hybrid Search (semantic 70% + keyword 30% with RRF Fusion)</constraint>
  <constraint type="search">Use German Full-Text Search (to_tsvector('german', content)) for better ranking</constraint>
  <constraint type="error-handling">Comprehensive error handling with try/except blocks and graceful degradation</constraint>
  <constraint type="optimization">Enhancement E12 implemented: German FTS (to_tsvector('german')) for better ranking of German queries</constraint>
  <constraint type="optimization">Enhancement E13 implemented: Comprehensive error handling with graceful degradation and structured logging</constraint>
  <constraint type="logging">Structured logging with logger.error(), logger.exception() for production debugging</constraint>
</constraints>
  <interfaces>
    <interface name="Database Connection" kind="context_manager" signature="with get_connection() as conn:" path="mcp_server/db/connection.py" />
    <interface name="OpenAI Embeddings" kind="function" signature="get_embedding_with_retry(client: OpenAI, text: str) -> list[float]" path="mcp_server/external/openai_client.py" />
    <interface name="RRF Fusion" kind="function" signature="rrf_fusion(semantic_results: list[dict], keyword_results: list[dict], weights: dict) -> list[dict]" path="mcp_server/tools/__init__.py" />
    <interface name="pgvector Registration" kind="function" signature="register_vector(conn)" path="pgvector.psycopg2" />
    <interface name="Ground Truth Table" kind="database_table" signature="INSERT INTO ground_truth (query, expected_docs) VALUES (%s, %s)" path="mcp_server/db/migrations/001_initial_schema.sql" />
  </interfaces>
  <tests>
    <standards>Manual testing strategy (no automated testing required). Testing focuses on stratified query extraction validation, hybrid search accuracy, labeling interface functionality, progress tracking, and edge case handling. All tests must validate German Full-Text Search and error handling with graceful degradation.</standards>
    <locations>Run Streamlit app locally: `streamlit run streamlit_apps/ground_truth_labeling.py`. Test with real L0 Raw Memory data in PostgreSQL database.</locations>
    <ideas>
      <test idea="Query Extraction Stratification" ac="AC1">Extract 100 queries, verify 40% Short (1-2 sentences), 40% Medium (3-5), 20% Long (6+ sentences)" />
      <test idea="Temporal Diversity Validation" ac="AC1">Verify queries come from eligible sessions (3-5 queries per session)" />
      <test idea="Sentence Counting Accuracy" ac="AC1">Test sentence counting with all punctuation marks (.!?)" />
      <test idea="Hybrid Search Top-5 Results" ac="AC2">Query with "Was denke ich über Autonomie?" - verify top-5 relevant docs returned" />
      <test idea="Labeling Interface Functionality" ac="AC2">Label 10 queries manually, verify checkbox decisions save correctly" />
      <test idea="Progress Tracking UI" ac="AC3">Label 68 queries, verify progress bar shows "68/100 Queries gelabelt"" />
      <test idea="Stratification Balance Display" ac="AC3">Verify live stratification balance percentages (Short/Medium/Long)" />
      <test idea="Save & Continue Later" ac="AC3">Label 50 queries, save, restart app, verify resume from query 51" />
      <test idea="PostgreSQL Storage Validation" ac="all">Verify ground_truth table contains query and expected_docs array correctly" />
      <test idea="Edge Case - Empty L0" ac="all">Test with empty L0 Raw Memory, verify appropriate error message" />
      <test idea="Edge Case - No Relevant Docs" ac="all">Skip labeling query or mark warning when no docs marked relevant" />
      <test idea="Error Handling - API Failures" ac="all">Test graceful degradation when OpenAI API or database fails" />
    </ideas>
  </tests>
</story-context>
