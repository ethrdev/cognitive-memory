<story-context id="bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>2</epicId>
    <storyId>7</storyId>
    <title>End-to-End RAG Pipeline Testing</title>
    <status>drafted</status>
    <generatedAt>2025-11-16</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>bmad-docs/stories/2-7-end-to-end-rag-pipeline-testing.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>Entwickler</asA>
    <iWant>die komplette RAG-Pipeline end-to-end testen</iWant>
    <soThat>alle Komponenten korrekt zusammenspielen (Query → Retrieval → Generation → Evaluation → Reflexion)</soThat>
    <tasks>
- Task 1: Vorbereitung Test-Environment
  - Subtask 1.1: Verify MCP Server läuft und alle Tools/Resources verfügbar
  - Subtask 1.2: Verify L2 Insights existieren für Test-Queries
  - Subtask 1.3: Verify Haiku API funktioniert
  - Subtask 1.4: Verify Episode Memory leer oder enthält bekannte Test-Episodes

- Task 2: High Confidence Query Test
  - Subtask 2.1: Prepare High Confidence Test Query
  - Subtask 2.2: Execute Complete Pipeline für High Confidence Query
  - Subtask 2.3: Verify Pipeline-Steps für High Confidence
  - Subtask 2.4: Measure Performance für High Confidence Query
  - Subtask 2.5: Verify Logging für High Confidence Query

- Task 3: Low Confidence Query Test (mit Reflexion-Trigger)
  - Subtask 3.1: Prepare Low Confidence Test Query
  - Subtask 3.2: Execute Complete Pipeline für Low Confidence Query
  - Subtask 3.3: Verify Reflexion-Trigger für Low Confidence
  - Subtask 3.4: Verify Episode Memory Storage
  - Subtask 3.5: Measure Performance für Low Confidence Query
  - Subtask 3.6: Verify Logging für Low Confidence Query

- Task 4: Medium Confidence Query Test
  - Subtask 4.1: Prepare Medium Confidence Test Query
  - Subtask 4.2: Execute Pipeline für Medium Confidence Query
  - Subtask 4.3: Measure Performance

- Task 5: Episode Memory Retrieval Test (Similar Query)
  - Subtask 5.1: Prepare Similar Query (nach Low Confidence Test)
  - Subtask 5.2: Execute Pipeline mit Episode Memory Integration
  - Subtask 5.3: Verify Lesson Learned Integration

- Task 6: Performance Benchmarking
  - Subtask 6.1: Run 10 Queries für statistisch robuste Latency-Messung
  - Subtask 6.2: Calculate p50 und p95 Latency
  - Subtask 6.3: Identify Performance Bottlenecks (falls p95 >5s)

- Task 7: End-to-End Pipeline Documentation
  - Subtask 7.1: Document Test Results
  - Subtask 7.2: Document Pipeline-Step Logs</tasks>
  </story>

  <acceptanceCriteria>AC-2.7.1: Complete 9-Step Pipeline Execution
- Query Expansion: 3 semantische Varianten generiert (Original + Paraphrase + Perspektiv-Shift + Keyword-Fokus)
- Embedding: 4 Queries embedded via OpenAI text-embedding-3-small (1536 dimensions)
- Hybrid Search: 4× hybrid_search Tool-Call mit RRF Fusion → Top-5 Dokumente
- Episode Memory Check: memory://episode-memory Resource gelesen (Similarity >0.70)
- CoT Generation: Thought → Reasoning → Answer → Confidence
- Self-Evaluation: Haiku API Evaluation (Reward Score -1.0 bis +1.0)
- Reflexion (conditional): Falls Reward <0.3 → Haiku Reflexion (Problem + Lesson Format)
- Working Memory Update: update_working_memory Tool-Call (LRU Eviction bei >10 Items)
- User Response: Answer + Confidence + Sources angezeigt

AC-2.7.2: Performance Metrics
- End-to-End Latency: <5s (p95, NFR001)
- Breakdown per Pipeline-Schritt:
  - Query Expansion: <0.5s
  - Embedding: <0.5s (4 parallele Calls)
  - Hybrid Search: <1s (p95)
  - CoT Generation: 2-3s (längster Schritt)
  - Evaluation: <0.5s (Haiku API)
  - Reflexion (falls getriggert): <1s
  - Working Memory Update: <0.1s

AC-2.7.3: Test Scenarios
- High Confidence Expected: Query mit klarem Match (Top-1 Score >0.85, Confidence >0.8, kein Reflexion-Trigger)
- Medium Confidence Expected: Ambigue Query (Top-1 Score 0.7-0.85, Confidence 0.5-0.8, kein Reflexion-Trigger)
- Low Confidence Expected: Query ohne Match (Scores <0.7, Confidence <0.5, Reflexion-Trigger erwartet bei Reward <0.3)

AC-2.7.4: Pipeline Logging
- Alle 9 Schritte in PostgreSQL geloggt
- Structured JSON Logs mit Timestamps
- Latency per Step geloggt
- API Costs tracked (api_cost_log Tabelle)
- Post-Mortem Analysis möglich</acceptanceCriteria>

  <artifacts>
    <docs>
      <!-- Epic 2 Technical Specification -->
      <doc>
        <path>bmad-docs/tech-spec-epic-2.md</path>
        <title>Epic 2 Technical Specification - RAG Pipeline & Hybrid Calibration</title>
        <section>Story 2.7 Acceptance Criteria (lines 423-428)</section>
        <snippet>AC-2.7.1: 9 Pipeline-Schritte komplett (Expansion → Embedding → Hybrid Search → Episode Memory → CoT → Evaluation → Reflexion → Working Memory → Response). AC-2.7.2: End-to-End Latency &lt;5s (p95). AC-2.7.3: Test High/Medium/Low Confidence Szenarien. AC-2.7.4: Pipeline-Schritte in PostgreSQL geloggt für Post-Mortem Analysis.</snippet>
      </doc>
      <doc>
        <path>bmad-docs/tech-spec-epic-2.md</path>
        <title>Epic 2 Technical Specification</title>
        <section>End-to-End RAG Pipeline Sequence (lines 159-183)</section>
        <snippet>Complete 10-step pipeline flow: User Query → Query Expansion (3 Varianten) → OpenAI Embeddings (4 parallel) → Hybrid Search (4× mit RRF Fusion) → Episode Memory Load → CoT Generation → Self-Evaluation (Haiku) → Conditional Reflexion → Working Memory Update → User Response. Total Cost: ~€0.003 per Query.</snippet>
      </doc>
      <doc>
        <path>bmad-docs/tech-spec-epic-2.md</path>
        <title>Epic 2 Technical Specification</title>
        <section>Performance NFR001 (lines 218-232)</section>
        <snippet>End-to-End Latency &lt;5s (p95). Breakdown: Query Expansion &lt;0.5s, Hybrid Search &lt;1s (p95), CoT Generation 2-3s, Evaluation &lt;0.5s, Reflexion &lt;1s. Performance Monitoring: p50, p95, p99 Percentiles tracked.</snippet>
      </doc>
      <doc>
        <path>bmad-docs/tech-spec-epic-2.md</path>
        <title>Epic 2 Technical Specification</title>
        <section>Observability - Logging Requirements (lines 261-278)</section>
        <snippet>Strukturiertes JSON Logging für alle Pipeline-Schritte. API Call Logs mit Token Counts und Latency. Evaluation Results mit Reward Scores und Reasoning. Complete Pipeline Trace für jede Query. Performance Profiles bei Latency-Issues.</snippet>
      </doc>

      <!-- System Architecture -->
      <doc>
        <path>bmad-docs/architecture.md</path>
        <title>Cognitive Memory System v3.1.0-Hybrid - Architektur</title>
        <section>Daten-Fluss: Typische Query (lines 85-114)</section>
        <snippet>Detailed flow diagram showing all 10 pipeline steps with cost breakdown per step. Total Cost per Query: ~€0.003. End-to-End Latency breakdown provided.</snippet>
      </doc>
      <doc>
        <path>bmad-docs/architecture.md</path>
        <title>Cognitive Memory System Architecture</title>
        <section>MCP Tools & Resources (lines 332-356)</section>
        <snippet>7 MCP Tools (store_raw_dialogue, compress_to_l2_insight, hybrid_search, update_working_memory, store_episode, get_golden_test_results, store_dual_judge_scores). 5 MCP Resources (memory://l2-insights, memory://working-memory, memory://episode-memory, memory://l0-raw, memory://stale-memory).</snippet>
      </doc>
      <doc>
        <path>bmad-docs/architecture.md</path>
        <title>Cognitive Memory System Architecture</title>
        <section>Logging Approach (lines 390-418)</section>
        <snippet>JSON Structured Logging format with timestamp, level, component, message, metadata. Levels: ERROR, WARN, INFO, DEBUG. Destinations: systemd Journal (journalctl -u cognitive-memory-mcp), /var/log/cognitive-memory/mcp.log (7-day rotation).</snippet>
      </doc>

      <!-- Integration Guides -->
      <doc>
        <path>docs/reflexion-integration-guide.md</path>
        <title>Reflexion Integration Guide</title>
        <section>Complete Integration (403 lines)</section>
        <snippet>Trigger Logic: should_trigger_reflection(reward_score) returns True when &lt;0.3. Episode Memory Flow: generate_reflection() → parse → store_episode. Lesson Retrieval Pattern: memory://episode-memory before CoT Generation. Complete integration examples for Claude Code.</snippet>
      </doc>
      <doc>
        <path>docs/cot-generation-guide.md</path>
        <title>Chain-of-Thought Generation Guide</title>
        <section>CoT Framework Implementation</section>
        <snippet>Structured output format: Thought (initial intuition) → Reasoning (explicit justification with context + episodes) → Answer (final response) → Confidence (score based on retrieval quality).</snippet>
      </doc>
      <doc>
        <path>docs/query-expansion-guide.md</path>
        <title>Query Expansion Guide</title>
        <section>Query Expansion Logic</section>
        <snippet>Generates 3 semantic variants: Paraphrase (different wording, same meaning), Perspektiv-Shift (viewpoint change), Keyword-Fokus (extract core concepts). Total 4 queries (original + 3 variants) for embedding.</snippet>
      </doc>
      <doc>
        <path>docs/integration/evaluation-integration-guide.md</path>
        <title>Evaluation Integration Guide</title>
        <section>Self-Evaluation with Haiku API</section>
        <snippet>Haiku API evaluation returns Reward Score -1.0 to +1.0 with reasoning. Temperature 0.0 for deterministic evaluation. Cost: ~€0.001 per evaluation.</snippet>
      </doc>

      <!-- Previous Stories -->
      <doc>
        <path>bmad-docs/stories/2-6-reflexion-framework-mit-verbal-reinforcement-learning.md</path>
        <title>Story 2.6 - Reflexion Framework</title>
        <section>Completion Notes (lines 519-564)</section>
        <snippet>HaikuClient.generate_reflection() verfügbar in anthropic_client.py. Reflexion Integration Pattern dokumentiert. Episode Memory Tool ready (store_episode). Cost Infrastructure complete (api_cost_log). Story 2.7 can use complete Reflexion infrastructure with NO code changes.</snippet>
      </doc>
    </docs>
    <code>
      <!-- External API Clients -->
      <artifact>
        <path>mcp_server/external/anthropic_client.py</path>
        <kind>service</kind>
        <symbol>HaikuClient</symbol>
        <lines>25-414</lines>
        <reason>Haiku API client with evaluate_answer() (line 78) and generate_reflection() (line 228) methods. Critical for Steps 6-7 of pipeline (Evaluation and Reflexion).</reason>
      </artifact>
      <artifact>
        <path>mcp_server/external/anthropic_client.py</path>
        <kind>method</kind>
        <symbol>HaikuClient.evaluate_answer</symbol>
        <lines>78-227</lines>
        <reason>Self-Evaluation implementation (AC-2.7.1 Step 6). Returns Reward Score -1.0 to +1.0 with reasoning. Temperature 0.0 for deterministic evaluation. From Story 2.5.</reason>
      </artifact>
      <artifact>
        <path>mcp_server/external/anthropic_client.py</path>
        <kind>method</kind>
        <symbol>HaikuClient.generate_reflection</symbol>
        <lines>228-414</lines>
        <reason>Reflexion generation (AC-2.7.1 Step 7). Triggered when Reward &lt;0.3. Returns Problem + Lesson format. From Story 2.6. Story 2.7 tests this in Low Confidence scenario.</reason>
      </artifact>

      <!-- Reflexion Utilities -->
      <artifact>
        <path>mcp_server/utils/reflexion_utils.py</path>
        <kind>module</kind>
        <symbol>reflexion_utils</symbol>
        <lines>1-200</lines>
        <reason>Reflexion trigger logic and utilities. Contains should_trigger_reflection() function (line 100) that returns True when reward &lt; threshold (default 0.3). Critical for AC-2.7.1 Step 7 conditional trigger.</reason>
      </artifact>
      <artifact>
        <path>mcp_server/utils/reflexion_utils.py</path>
        <kind>function</kind>
        <symbol>should_trigger_reflection</symbol>
        <lines>100-141</lines>
        <reason>Determines if reflexion should be triggered based on reward score. Default threshold: 0.3 (from config.yaml). Used in Low Confidence test scenario to verify conditional reflexion.</reason>
      </artifact>

      <!-- Retry Logic -->
      <artifact>
        <path>mcp_server/utils/retry_logic.py</path>
        <kind>module</kind>
        <symbol>retry_logic</symbol>
        <lines>1-150</lines>
        <reason>Exponential backoff retry logic for API calls (OpenAI, Anthropic). Critical for handling API failures during pipeline execution. From Story 2.4.</reason>
      </artifact>

      <!-- Database and Logging -->
      <artifact>
        <path>mcp_server/db/connection.py</path>
        <kind>module</kind>
        <symbol>connection</symbol>
        <lines>1-100</lines>
        <reason>PostgreSQL connection pool for database operations. Used for logging pipeline steps, costs, and storing episode memory. Critical for AC-2.7.4 (Pipeline Logging).</reason>
      </artifact>
      <artifact>
        <path>mcp_server/db/evaluation_logger.py</path>
        <kind>module</kind>
        <symbol>evaluation_logger</symbol>
        <lines>1-150</lines>
        <reason>Evaluation result logging to PostgreSQL. Logs reward scores, evaluation reasoning, and timestamps for post-mortem analysis. Critical for AC-2.7.4.</reason>
      </artifact>

      <!-- MCP Tools (to be tested) -->
      <artifact>
        <path>mcp_server/tools/dual_judge.py</path>
        <kind>mcp_tool</kind>
        <symbol>store_dual_judge_scores</symbol>
        <lines>1-200</lines>
        <reason>MCP tool for storing dual judge scores (GPT-4o + Haiku). From Story 1.11. Not directly used in Story 2.7, but part of overall MCP server infrastructure.</reason>
      </artifact>

      <!-- Note: MCP Tools for Stories 1.4-1.8 -->
      <!-- Expected tools that Story 2.7 will test (may not all be implemented yet):
           - store_raw_dialogue (Story 1.4)
           - compress_to_l2_insight (Story 1.5)
           - hybrid_search (Story 1.6) - Critical for AC-2.7.1 Step 3
           - update_working_memory (Story 1.7) - Critical for AC-2.7.1 Step 8
           - store_episode (Story 1.8) - Critical for AC-2.7.1 Step 7 (reflexion storage)
           - get_golden_test_results (Story 3.2) - Not in scope for Story 2.7
      -->
    </code>
    <dependencies>
      <python>
        <package name="anthropic" version="latest">Anthropic Python SDK for Haiku API (evaluation and reflexion)</package>
        <package name="openai" version="latest">OpenAI Python SDK for embeddings (text-embedding-3-small)</package>
        <package name="psycopg2" version="latest">PostgreSQL adapter for Python (database connection and logging)</package>
        <package name="mcp" version="latest">MCP (Model Context Protocol) SDK for tool/resource framework</package>
      </python>
      <database>
        <system name="PostgreSQL" version="15+">Primary database for L0/L2 memory, working memory, episode memory, and logging</system>
        <extension name="pgvector" version="latest">Vector similarity search for semantic retrieval (AC-2.7.1 Step 3)</extension>
      </database>
      <apis>
        <api name="OpenAI Embeddings API" model="text-embedding-3-small">1536-dimensional embeddings for queries and L2 insights. Cost: ~€0.00008 for 4 embeddings per query.</api>
        <api name="Anthropic Claude API" model="claude-3-5-haiku-20241022">Evaluation (€0.001/call) and Reflexion (€0.0015/call). Temperature 0.0 for eval, 0.7 for reflexion.</api>
      </apis>
    </dependencies>
  </artifacts>

  <constraints>
    <constraint type="implementation">Story 2.7 is TESTING ONLY - NO code changes allowed. All components (Stories 2.1-2.6) must already be implemented.</constraint>
    <constraint type="testing">Manual testing in Claude Code interface required. No automated test suite - exploratory testing approach.</constraint>
    <constraint type="performance">End-to-End Latency MUST be &lt;5s (p95) per NFR001. This is a critical acceptance criterion.</constraint>
    <constraint type="scenarios">MUST test all 3 scenarios: High Confidence (Top-1 Score &gt;0.85), Medium Confidence (Score 0.7-0.85), Low Confidence (Score &lt;0.7 with Reflexion trigger).</constraint>
    <constraint type="logging">ALL 9 pipeline steps MUST be logged to PostgreSQL with structured JSON format for post-mortem analysis (AC-2.7.4).</constraint>
    <constraint type="cost">Pipeline cost must stay within budget: ~€0.003 per query (€3/mo at 1000 queries). Track via api_cost_log table.</constraint>
    <constraint type="reflexion">Reflexion MUST trigger when Reward &lt;0.3 (configurable threshold). Test with Low Confidence scenario.</constraint>
    <constraint type="episode-memory">Episode Memory storage via store_episode tool MUST work. Verify retrieval with similar query test.</constraint>
    <constraint type="infrastructure">MCP Server must be running with all 7 tools and 5 resources available. Verify in Task 1 (Test Environment Preparation).</constraint>
  </constraints>

  <interfaces>
    <!-- MCP Tools (7 total) -->
    <interface>
      <name>hybrid_search</name>
      <kind>MCP Tool</kind>
      <signature>hybrid_search(query_embedding: vector, query_text: str, top_k: int = 5, semantic_weight: float = 0.8, keyword_weight: float = 0.2) -> List[Dict]</signature>
      <path>mcp_server/tools/hybrid_search.py</path>
      <reason>Critical for AC-2.7.1 Step 3. Called 4× in parallel (one per query variant). Returns Top-5 documents using RRF Fusion of semantic + keyword search.</reason>
    </interface>
    <interface>
      <name>update_working_memory</name>
      <kind>MCP Tool</kind>
      <signature>update_working_memory(content: str, importance: float = 0.5) -> Dict[str, Any]</signature>
      <path>mcp_server/tools/update_working_memory.py</path>
      <reason>AC-2.7.1 Step 8. LRU eviction when &gt;10 items. Returns evicted_item if eviction occurred.</reason>
    </interface>
    <interface>
      <name>store_episode</name>
      <kind>MCP Tool</kind>
      <signature>store_episode(query: str, reward: float, reflection: str) -> Dict[str, Any]</signature>
      <path>mcp_server/tools/store_episode.py</path>
      <reason>AC-2.7.1 Step 7 (conditional). Stores reflexion in episode_memory table with query embedding. Critical for Low Confidence scenario test.</reason>
    </interface>
    <interface>
      <name>store_raw_dialogue</name>
      <kind>MCP Tool</kind>
      <signature>store_raw_dialogue(session_id: str, speaker: str, content: str, metadata: Dict) -> Dict</signature>
      <path>mcp_server/tools/store_raw_dialogue.py</path>
      <reason>L0 storage for raw dialogue. Part of overall pipeline but not directly tested in Story 2.7.</reason>
    </interface>
    <interface>
      <name>compress_to_l2_insight</name>
      <kind>MCP Tool</kind>
      <signature>compress_to_l2_insight(content: str, source_ids: List[int]) -> Dict</signature>
      <path>mcp_server/tools/compress_to_l2_insight.py</path>
      <reason>L2 insight creation with embedding. Part of infrastructure but not directly tested in Story 2.7.</reason>
    </interface>

    <!-- MCP Resources (5 total) -->
    <interface>
      <name>memory://episode-memory</name>
      <kind>MCP Resource</kind>
      <signature>GET memory://episode-memory?query={q}&amp;min_similarity={threshold}</signature>
      <path>mcp_server/resources/episode_memory.py</path>
      <reason>AC-2.7.1 Step 4. Retrieves similar past episodes (Similarity &gt;0.70). Returns Lesson Learned for CoT integration. Critical for Similar Query test.</reason>
    </interface>
    <interface>
      <name>memory://l2-insights</name>
      <kind>MCP Resource</kind>
      <signature>GET memory://l2-insights?query={q}&amp;top_k={k}</signature>
      <path>mcp_server/resources/l2_insights.py</path>
      <reason>Read L2 insights for verification in Task 1. Ensures test data exists before running pipeline tests.</reason>
    </interface>
    <interface>
      <name>memory://working-memory</name>
      <kind>MCP Resource</kind>
      <signature>GET memory://working-memory</signature>
      <path>mcp_server/resources/working_memory.py</path>
      <reason>Read current working memory state. Used for verification after update_working_memory calls.</reason>
    </interface>

    <!-- Python API Interfaces -->
    <interface>
      <name>HaikuClient.evaluate_answer</name>
      <kind>Python Method</kind>
      <signature>async evaluate_answer(query: str, context: List[str], answer: str) -> Tuple[float, str]</signature>
      <path>mcp_server/external/anthropic_client.py:78</path>
      <reason>AC-2.7.1 Step 6. Returns (reward_score, reasoning). Temperature 0.0 for deterministic evaluation.</reason>
    </interface>
    <interface>
      <name>HaikuClient.generate_reflection</name>
      <kind>Python Method</kind>
      <signature>async generate_reflection(query: str, answer: str, context: List[str], reward: float, evaluation_reasoning: str) -> str</signature>
      <path>mcp_server/external/anthropic_client.py:228</path>
      <reason>AC-2.7.1 Step 7. Returns Problem + Lesson format. Triggered when reward &lt;0.3.</reason>
    </interface>
    <interface>
      <name>should_trigger_reflection</name>
      <kind>Python Function</kind>
      <signature>should_trigger_reflection(reward_score: float, threshold: Optional[float] = None) -> bool</signature>
      <path>mcp_server/utils/reflexion_utils.py:100</path>
      <reason>Determines reflexion trigger. Default threshold 0.3 from config.yaml. Critical for Low Confidence test.</reason>
    </interface>

    <!-- Database Tables -->
    <interface>
      <name>api_cost_log</name>
      <kind>PostgreSQL Table</kind>
      <signature>INSERT INTO api_cost_log (date, api_name, num_calls, token_count, estimated_cost)</signature>
      <path>Database Schema</path>
      <reason>AC-2.7.4. Tracks all API costs (embeddings, evaluation, reflexion). Critical for budget validation.</reason>
    </interface>
    <interface>
      <name>episode_memory</name>
      <kind>PostgreSQL Table</kind>
      <signature>INSERT INTO episode_memory (query, reward, reflection, embedding)</signature>
      <path>Database Schema</path>
      <reason>AC-2.7.1 Step 7. Stores reflexions with query embeddings for future retrieval.</reason>
    </interface>
  </interfaces>
  <tests>
    <standards>
Story 2.7 uses MANUAL EXPLORATORY TESTING approach in Claude Code interface. No automated test suite. Testing methodology:
- Interactive testing through Claude Code UI with live MCP Server
- Manual verification of all 9 pipeline steps for each test scenario
- Performance measurement using manual timing (start/stop timer approach)
- Visual inspection of logs in PostgreSQL via psql or database client
- Post-mortem analysis of structured JSON logs from systemd journal (journalctl -u cognitive-memory-mcp)

Testing Framework: N/A (manual testing only, no pytest/unittest)
Logging Standard: JSON Structured Logging (timestamp, level, component, message, metadata)
Performance Measurement: Manual latency tracking per pipeline step, calculation of p50/p95 percentiles from 10-query benchmark
Cost Tracking: Query api_cost_log table after each test to verify cost tracking works

Quality Gates:
- All 9 pipeline steps must execute successfully for each scenario
- Latency p95 must be &lt;5s (hard requirement from NFR001)
- Reflexion must trigger for Low Confidence scenario (Reward &lt;0.3)
- Episode Memory must store and retrieve lessons correctly
- All steps must be logged to PostgreSQL with complete metadata
    </standards>
    <locations>
Manual testing locations:
- Claude Code Interface: Primary testing environment for end-to-end pipeline execution
- PostgreSQL Database: For verifying logs, episode memory, working memory, and cost tracking
  - Tables to check: api_cost_log, episode_memory, working_memory, l2_insights
  - Access via: psql or database client
- systemd Journal: For pipeline step logs
  - Command: journalctl -u cognitive-memory-mcp --since "10 minutes ago"
- Test Results Documentation: bmad-docs/testing/story-2-7-pipeline-test-results.md (to be created in Task 7)

No test files required - this is a manual integration test story.
    </locations>
    <ideas>
<!-- Test scenarios mapped to Acceptance Criteria -->

AC-2.7.1 (9-Step Pipeline Execution):
- Test Idea 1: Execute complete pipeline with High Confidence query and verify all 9 steps complete successfully
  - Verify: Query Expansion generates 4 queries (original + 3 variants)
  - Verify: Embedding API called 4 times in parallel
  - Verify: hybrid_search tool called 4 times with RRF Fusion
  - Verify: Episode Memory resource queried (may return empty initially)
  - Verify: CoT Generation produces Thought + Reasoning + Answer + Confidence
  - Verify: Haiku evaluate_answer returns Reward Score + Reasoning
  - Verify: Reflexion NOT triggered (High Confidence expects Reward &gt;0.3)
  - Verify: update_working_memory called successfully
  - Verify: User receives Answer + Confidence + Sources (L2 IDs)

- Test Idea 2: Execute pipeline with Low Confidence query to trigger Reflexion
  - Setup: Use query unrelated to L2 Insights content (e.g., "What is the capital of Sweden?")
  - Verify: All Retrieval Scores &lt;0.7 (poor semantic match)
  - Verify: CoT Confidence &lt;0.5
  - Verify: Haiku Evaluation returns Reward &lt;0.3
  - Verify: should_trigger_reflection() returns True
  - Verify: HaikuClient.generate_reflection() called
  - Verify: Reflexion text contains Problem + Lesson format
  - Verify: store_episode tool stores reflexion to episode_memory table
  - Verify: Episode stored with query embedding for future retrieval

- Test Idea 3: Execute pipeline with similar query to verify Episode Memory retrieval
  - Prerequisites: Low Confidence test must run first to create episode
  - Setup: Use query similar to previous Low Confidence query (Similarity &gt;0.70)
  - Verify: memory://episode-memory resource returns previous episode
  - Verify: Lesson Learned integrated into CoT Reasoning
  - Verify: User sees transparent reference to past experience (NFR005 Transparency)

AC-2.7.2 (Performance Metrics):
- Test Idea 4: Measure end-to-end latency for 10 queries (4 High, 4 Medium, 2 Low Confidence)
  - Measure: Total time from query submission to user response
  - Record: Individual latencies for all 10 queries
  - Calculate: p50 (median) and p95 (95th percentile) latency
  - Verify: p95 &lt;5s (critical NFR001 requirement)
  - If p95 &gt;5s: Identify bottlenecks (CoT Generation likely culprit at 2-3s)

- Test Idea 5: Measure latency breakdown per pipeline step
  - Measure each step individually (Query Expansion, Embedding, Hybrid Search, etc.)
  - Compare actual vs expected latencies from tech spec
  - Expected: Query Expansion &lt;0.5s, Embedding &lt;0.5s, Hybrid Search &lt;1s, CoT 2-3s, Evaluation &lt;0.5s, Reflexion &lt;1s

AC-2.7.3 (Test Scenarios Coverage):
- Test Idea 6: Prepare and execute High Confidence scenario
  - Find query with strong match in L2 Insights (Top-1 Score &gt;0.85)
  - Expected: Confidence &gt;0.8, NO Reflexion trigger, Latency ~3s

- Test Idea 7: Prepare and execute Medium Confidence scenario
  - Find ambiguous query (Top-1 Score 0.7-0.85)
  - Expected: Confidence 0.5-0.8, NO Reflexion trigger, Latency ~3s

- Test Idea 8: Prepare and execute Low Confidence scenario
  - Use query with no match in L2 Insights (All Scores &lt;0.7)
  - Expected: Confidence &lt;0.5, Reflexion TRIGGERED, Latency ~4-5s

AC-2.7.4 (Pipeline Logging):
- Test Idea 9: Verify all 9 steps logged to PostgreSQL
  - Check: systemd journal logs (journalctl -u cognitive-memory-mcp)
  - Verify: JSON format with timestamp, level, component, message, metadata
  - Verify: Each pipeline step has corresponding log entry
  - Verify: Latency logged for each step

- Test Idea 10: Verify API cost tracking
  - Query api_cost_log table after test queries
  - Verify: Embedding costs logged (api_name='openai_embeddings')
  - Verify: Evaluation costs logged (api_name='haiku_eval')
  - Verify: Reflexion costs logged (api_name='haiku_reflexion') for Low Confidence queries
  - Verify: Token counts and estimated costs are reasonable

- Test Idea 11: Verify post-mortem analysis capability
  - Simulate scenario: "Why was this query slow?"
  - Use logs to trace latency breakdown per step
  - Verify: Can identify bottleneck step from logs
  - Verify: Can reconstruct complete pipeline execution from logs
    </ideas>
  </tests>
</story-context>
