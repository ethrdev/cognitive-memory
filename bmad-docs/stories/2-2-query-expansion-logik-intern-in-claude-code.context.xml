<story-context id="bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>2</epicId>
    <storyId>2</storyId>
    <title>Query Expansion Logik intern in Claude Code</title>
    <status>drafted</status>
    <generatedAt>2025-11-16</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>bmad-docs/stories/2-2-query-expansion-logik-intern-in-claude-code.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>Claude Code</asA>
    <iWant>User-Queries intern in 3 semantische Varianten reformulieren</iWant>
    <soThat>robuste Retrieval mit +10-15% Recall Uplift möglich ist (ohne externe API-Kosten)</soThat>
    <tasks>
- [ ] Document Query Expansion Pattern & Strategy (AC: 1, 4)
  - [ ] Erstelle `/docs/query-expansion-guide.md` mit Expansion-Strategie-Dokumentation
  - [ ] Dokumentiere alle 3 Varianten-Typen mit Beispielen
  - [ ] Definiere Prompt-Pattern für Claude Code (wie Varianten generieren)
  - [ ] Dokumentiere Cost-Savings (€0 vs. Haiku API €0.50/Query)

- [ ] Configure Query Expansion Parameters (AC: 1, 3)
  - [ ] Erweitere `config.yaml` mit `query_expansion` Section
  - [ ] Definiere `enabled: true`, `num_variants: 3`, `strategies: [paraphrase, perspective_shift, keyword_focus]`
  - [ ] Dokumentiere Konfiguration-Options (2-5 Varianten)
  - [ ] Verifiziere Config wird korrekt von MCP Server geladen (für Logging/Tracking)

- [ ] Implement Result Deduplication Logic (AC: 2, 3)
  - [ ] Erstelle `mcp_server/utils/query_expansion.py` mit Deduplication-Helper
  - [ ] Implementiere `deduplicate_by_l2_id(search_results: List[SearchResult]) -> List[SearchResult]`
  - [ ] Implementiere `merge_rrf_scores(results: List[SearchResult]) -> List[SearchResult]`
  - [ ] Verifiziere Top-5 Finale Results nach RRF Fusion

- [ ] Test Query Expansion End-to-End (AC: alle)
  - [ ] Test mit 5 Sample Queries (Short, Medium, Long Mix)
  - [ ] Verifiziere 3 Varianten pro Query generiert werden
  - [ ] Verifiziere alle 4 Queries (Original + 3 Varianten) embedded werden (4 OpenAI API Calls)
  - [ ] Verifiziere alle 4 Queries an `hybrid_search` geschickt werden (4 MCP Tool Calls)
  - [ ] Verifiziere Deduplication funktioniert (keine doppelten L2 IDs)
  - [ ] Verifiziere RRF Fusion korrekt merged (Top-5 Final Results)
  - [ ] Messe Latency (+0.5-1s für Expansion akzeptabel)

- [ ] Document Recall Uplift & Performance (AC: alle)
  - [ ] Führe Baseline Test durch (Retrieval OHNE Query Expansion)
  - [ ] Führe Expansion Test durch (Retrieval MIT Query Expansion)
  - [ ] Berechne Recall Uplift (+10-15% erwartet)
  - [ ] Dokumentiere Performance-Impact (Latency, Token-Cost, API-Calls)
  - [ ] Dokumentiere in `/docs/query-expansion-evaluation.md`
    </tasks>
  </story>

  <acceptanceCriteria>
**AC-2.2.1: Query Variant Generation**
**Given** eine User-Query wird gestellt
**When** Query Expansion durchgeführt wird
**Then** werden 3 Varianten generiert:
1. **Original Query:** Unveränderte User-Frage
2. **Variante 1 (Paraphrase):** Andere Wortwahl, gleiche Bedeutung
3. **Variante 2 (Perspektiv-Shift):** z.B. "Was denke ich..." → "Meine Meinung zu..."
4. **Variante 3 (Keyword-Fokus):** Extrahiere Kern-Konzepte und Keywords

**AC-2.2.2: Retrieval with All Variants**
**And** alle 4 Queries (Original + 3 Varianten) werden für Retrieval genutzt:
- Jede Query wird embedded via OpenAI API (4 Embedding-Calls)
- Jede Query wird an `hybrid_search` MCP Tool geschickt (4 Tool-Calls)
- Ergebnisse werden merged und dedupliziert (nach L2 Insight ID)
- Finale Top-5 Dokumente via Reciprocal Rank Fusion (RRF)

**AC-2.2.3: Configurable Expansion Strategy**
**And** Expansion-Strategie ist konfigurierbar:
- **Default:** 3 Varianten (Balance zwischen Recall und Token-Cost)
- **Optional:** 2 Varianten (Low-Budget Mode)
- **Optional:** 5 Varianten (High-Recall Mode)
- Konfiguration in `config.yaml` unter `query_expansion.num_variants`

**AC-2.2.4: Zero External API Costs**
**And** keine externen API-Kosten entstehen:
- Expansion läuft intern in Claude Code (Teil des Reasoning-Prozesses)
- Keine separaten API-Calls nötig (€0/mo vs. €0.50/mo hätte Haiku API gebraucht)
- Token-Cost: ~200 Tokens für Expansion (vernachlässigbar in Claude MAX Subscription)
  </acceptanceCriteria>

  <artifacts>
    <docs>
      <artifact path="bmad-docs/tech-spec-epic-2.md" title="Epic 2 Technical Specification: RAG Pipeline & Hybrid Calibration" section="Query Expansion Integration" snippet="Query Expansion Process: Original Query → Paraphrase → Perspective Shift → Keyword Focus. Expansion in Claude Code (internal reasoning). Cost savings: €0 vs €0.50/query (Haiku API avoided). Token cost: ~200 tokens (negligible in Claude MAX)." />
      <artifact path="bmad-docs/tech-spec-epic-2.md" title="Epic 2 Technical Specification" section="Workflows and Sequencing - End-to-End RAG Pipeline" snippet="Step 2: Query Expansion (intern): Original Query, Paraphrase Variant, Perspective Shift Variant, Keyword Focus Variant. Step 3: OpenAI Embeddings API (parallel, 4 Queries). Step 4: MCP Tool: hybrid_search (4× parallel, RRF Fusion)." />
      <artifact path="bmad-docs/tech-spec-epic-2.md" title="Epic 2 Technical Specification" section="Configuration Dependencies" snippet="query_expansion: enabled: true, num_variants: 3, strategies: [paraphrase, perspective_shift, keyword_focus]. Config stored in config.yaml with query_expansion section." />
      <artifact path="bmad-docs/epics.md" title="Epic Breakdown" section="Story 2.2: Query Expansion Logik" snippet="Query expansion runs internally in Claude Code (€0/mo vs €0.50/query with Haiku API). 4 parallel queries (original + 3 variants) → hybrid_search calls. Deduplication + RRF fusion for Top-5 final results. Expected +10-15% recall uplift." />
      <artifact path="bmad-docs/architecture.md" title="System Architecture" section="Implementierungs-Patterns - RRF Fusion" snippet="RRF Formula: score = Σ 1/(k + rank_i) with k=60 (Standard). Used to merge results from 4 query variants into single ranked set. Implementation in mcp_server/utils/rrf_fusion.py." />
      <artifact path="bmad-docs/PRD.md" title="Product Requirements Document" section="FR005: Query Expansion for robust search" snippet="Claude Code reformulates search queries internally in 3 semantic variants (replaces Haiku API Call, €0/mo instead of €8.50/mo), calls hybrid_search for all variants, merges results." />
      <artifact path="bmad-docs/stories/2-1-claude-code-mcp-client-setup-integration-testing.md" title="Story 2.1 - Claude Code MCP Client Setup" section="Learnings from Previous Story" snippet="MCP Server integration working. All 7 tools and 5 resources registered successfully. hybrid_search tool will be called 4× in this story (once per query variant). OpenAI API keys must be configured for embedding generation." />
    </docs>
    <code>
      <artifact path="mcp_server/tools/__init__.py" kind="module" symbol="hybrid_search" lines="608-714" reason="Existing hybrid_search tool that will be called 4 times (once per query variant). Implements semantic + keyword search with RRF fusion. Returns top-k results with scores." />
      <artifact path="mcp_server/tools/__init__.py" kind="function" symbol="rrf_fusion" lines="34-96" reason="Reciprocal Rank Fusion implementation used to merge results from multiple queries. Formula: score = weight/(k + rank) with k=60. Story 2.2 will reuse this pattern for deduplicating and fusing 4 query variant results." />
      <artifact path="mcp_server/tools/__init__.py" kind="function" symbol="get_embedding_with_retry" lines="343-403" reason="OpenAI embeddings API call with exponential backoff retry logic. Will be called 4 times in parallel for each query variant. Cost: €0.00002 per embedding." />
      <artifact path="mcp_server/db/connection.py" kind="module" symbol="get_connection" lines="96-151" reason="Database connection context manager. Returns connection with DictCursor. Used pattern: 'with get_connection() as conn:'. Story 2.2 may need for logging/tracking expansion calls." />
      <artifact path="config/config.yaml" kind="config" symbol="base.memory" lines="12-22" reason="Existing config structure for memory system. Story 2.2 will add 'query_expansion' section here with: enabled, num_variants, strategies, parallel_embedding, parallel_search, rrf_k, final_top_k." />
    </code>
    <dependencies>
      <python>
        <dependency name="mcp" version="^1.0.0" purpose="MCP server framework (stdio transport)" />
        <dependency name="psycopg2-binary" version="^2.9.0" purpose="PostgreSQL database driver" />
        <dependency name="pgvector" version="^0.2.0" purpose="Vector similarity search extension" />
        <dependency name="openai" version="^1.0.0" purpose="OpenAI API client for embeddings (text-embedding-3-small, 1536-dim)" />
        <dependency name="anthropic" version="^0.25.0" purpose="Anthropic API client (not used in Story 2.2 - expansion is internal)" />
        <dependency name="numpy" version="^1.24.0" purpose="Numerical operations (embedding manipulation)" />
        <dependency name="python-dotenv" version="^1.0.0" purpose="Environment variable management (.env files)" />
      </python>
      <system>
        <dependency name="PostgreSQL" version="15+" purpose="Primary database with pgvector extension" />
        <dependency name="Claude Code" version="Sonnet 4.5" purpose="Internal query expansion execution (€0/mo in MAX subscription)" />
      </system>
    </dependencies>
  </artifacts>

  <constraints>
    <development>
      <constraint>Query expansion MUST run internally in Claude Code (no external API calls) to maintain €0/mo cost target for bulk operations</constraint>
      <constraint>All 4 queries (original + 3 variants) MUST be embedded in parallel to minimize latency</constraint>
      <constraint>All 4 queries MUST call hybrid_search in parallel via MCP protocol (4 concurrent tool calls)</constraint>
      <constraint>Deduplication MUST happen by L2 Insight ID to prevent duplicate documents in final Top-5 results</constraint>
      <constraint>RRF fusion MUST use k=60 constant (standard in literature) for reciprocal rank formula</constraint>
      <constraint>Final results MUST be limited to Top-5 documents (not Top-10) to minimize context window risk</constraint>
      <constraint>Token cost for expansion should be ~200 tokens per query (acceptable in Claude MAX subscription)</constraint>
      <constraint>Latency added by expansion should be +0.5-1s (acceptable within 5s p95 budget from NFR001)</constraint>
    </development>
    <architecture>
      <constraint>Query expansion is an INTERNAL reasoning step, not a separate MCP Tool or external API</constraint>
      <constraint>Configuration must live in config.yaml under 'query_expansion' section (not hardcoded)</constraint>
      <constraint>Deduplication logic must be in mcp_server/utils/query_expansion.py (new file)</constraint>
      <constraint>RRF fusion should reuse existing rrf_fusion() pattern from mcp_server/tools/__init__.py</constraint>
      <constraint>No changes to MCP server core or tool registration required (expansion happens client-side in Claude Code)</constraint>
    </architecture>
    <testing>
      <constraint>Manual testing required in Claude Code interface (no automated unit tests for internal reasoning)</constraint>
      <constraint>Must test with 5 sample queries: Short, Medium, Long, Single-Word, Deduplication edge case</constraint>
      <constraint>Must measure recall uplift (+10-15% expected) vs. baseline (single query without expansion)</constraint>
      <constraint>Must verify latency remains <5s p95 end-to-end (expansion adds ~0.5-1s)</constraint>
    </testing>
  </constraints>

  <interfaces>
    <mcp-tool name="hybrid_search" kind="MCP Tool (existing)" signature="hybrid_search(query_embedding: list[float], query_text: str, top_k: int = 5, weights: dict = {semantic: 0.7, keyword: 0.3})" path="mcp_server/tools/__init__.py:608-714" notes="Will be called 4 times in parallel (once per query variant). Returns list of search results with L2 ID, content, score, source_ids. Critical: Must handle concurrent calls correctly." />
    <api name="OpenAI Embeddings API" kind="External REST API" signature="client.embeddings.create(model='text-embedding-3-small', input=text) -> list[float]" path="mcp_server/tools/__init__.py:343-403" notes="Called 4 times in parallel for each query variant. Cost: €0.00002 per embedding (€0.00008 total per query). Includes retry logic with exponential backoff (1s, 2s, 4s)." />
    <function name="deduplicate_by_l2_id" kind="Python utility function (new)" signature="deduplicate_by_l2_id(search_results: List[Dict]) -> List[Dict]" path="mcp_server/utils/query_expansion.py" notes="NEW function to create. Deduplicates search results by L2 Insight ID. Keeps highest-scoring result per ID. Input: Combined results from 4 queries. Output: Unique results sorted by score." />
    <function name="merge_rrf_scores" kind="Python utility function (new)" signature="merge_rrf_scores(results_list: List[List[Dict]], k: int = 60) -> List[Dict]" path="mcp_server/utils/query_expansion.py" notes="NEW function to create. Merges 4 result lists using Reciprocal Rank Fusion. Formula: score(doc) = Σ 1/(k + rank_i). Returns Top-5 documents after fusion." />
  </interfaces>

  <tests>
    <standards>Manual testing in Claude Code interface (no automated unit tests). Story 2.2 focuses on internal reasoning behavior, which cannot be unit-tested directly. Testing strategy includes: End-to-end testing with sample queries, Performance benchmarking (latency + recall uplift), Cost validation (€0 for expansion, €0.00008 total per query). Success criteria: All 5 test cases pass, Latency <5s p95, Recall uplift +10-15%.</standards>
    <locations>
      - docs/query-expansion-guide.md (documentation for expansion patterns, to be created)
      - docs/query-expansion-evaluation.md (recall uplift measurements, to be created)
      - mcp_server/utils/query_expansion.py (deduplication + RRF fusion helpers, to be created)
    </locations>
    <ideas>
      <idea ac="AC-2.2.1" description="Test query variant generation with diverse query types: Short query ('Was denke ich über Autonomie?'), Medium query ('Wie verstehe ich die Beziehung zwischen Bewusstsein und Identität?'), Long query (6+ sentences), Single-word query ('Bewusstsein'). Verify 3 variants generated for each (Paraphrase, Perspective Shift, Keyword Focus)." />
      <idea ac="AC-2.2.2" description="Test parallel embedding and hybrid_search calls. Verify 4 OpenAI API calls (one per variant). Verify 4 hybrid_search MCP tool calls (concurrent). Measure total latency (should be ~0.8-1.0s for parallel search, not 3-4s sequential)." />
      <idea ac="AC-2.2.3" description="Test deduplication logic. Manually verify results from 4 queries have overlapping documents. Verify deduplicate_by_l2_id() removes duplicates. Verify final Top-5 contains unique L2 IDs only." />
      <idea ac="AC-2.2.4" description="Test RRF fusion correctness. Verify merge_rrf_scores() uses k=60 constant. Verify scores calculated correctly: score = Σ 1/(60 + rank). Verify Top-5 ranking matches RRF formula (not simple average)." />
      <idea ac="AC-2.2.1,AC-2.2.2,AC-2.2.3,AC-2.2.4" description="End-to-end recall uplift test. Baseline: Single query without expansion → measure Precision@5. Expansion: 4 queries with fusion → measure Precision@5. Calculate uplift (+10-15% expected). Document in docs/query-expansion-evaluation.md." />
      <idea ac="AC-2.2.4" description="Cost validation test. Verify no Haiku API calls triggered (expansion is internal). Verify exactly 4 OpenAI API calls (embeddings). Verify total cost €0.00008 per query (4 embeddings × €0.00002). Document cost savings: €0.00008 vs €0.50008 if Haiku API used." />
    </ideas>
  </tests>
</story-context>
